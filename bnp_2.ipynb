{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost and GA with GLPK\n",
    "This notebook contains the implementation of a Genetic Algorithm to carry out feature selection. This hasn't produced a reasonable result however (in fact, it can't find a tree), as it appears to have selected too many features for use with BNP-OCT (35 features currently)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import loadtxt\n",
    "import pandas as pd\n",
    "from numpy import sort\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "\n",
    "from lsopt.tree import OptimalTreeClassifier ## M-OCT propsed by Liu & Allen\n",
    "from lsopt.tree import BinNodePenaltyOptimalTreeClassifier ## BNP-OCT propsed by Liu & Allen\n",
    "# from lsopt.tree import OldOptimalTreeClassifier ## OCT proposed by Bertsimas & Dunn\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import tree \n",
    "\n",
    "import graphviz\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phy_same</th>\n",
       "      <th>phy_count</th>\n",
       "      <th>period</th>\n",
       "      <th>periodadmt</th>\n",
       "      <th>age</th>\n",
       "      <th>alife</th>\n",
       "      <th>provider_InscClaimAmtReimbursed_mean</th>\n",
       "      <th>provider_DeductibleAmtPaid_mean</th>\n",
       "      <th>provider_NoOfMonths_PartACov_mean</th>\n",
       "      <th>provider_NoOfMonths_PartBCov_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>diag1_OPAnnualDeductibleAmt_mean</th>\n",
       "      <th>diag1_InscClaimAmtReimbursed_std</th>\n",
       "      <th>diag1_DeductibleAmtPaid_std</th>\n",
       "      <th>diag1_NoOfMonths_PartACov_std</th>\n",
       "      <th>diag1_NoOfMonths_PartBCov_std</th>\n",
       "      <th>diag1_IPAnnualReimbursementAmt_std</th>\n",
       "      <th>diag1_IPAnnualDeductibleAmt_std</th>\n",
       "      <th>diag1_OPAnnualReimbursementAmt_std</th>\n",
       "      <th>diag1_OPAnnualDeductibleAmt_std</th>\n",
       "      <th>PotentialFraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.960000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>1.440000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>80.240000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4185.600000</td>\n",
       "      <td>213.600000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>543.045084</td>\n",
       "      <td>3482.066310</td>\n",
       "      <td>161.353027</td>\n",
       "      <td>0.569945</td>\n",
       "      <td>0.424192</td>\n",
       "      <td>12941.552350</td>\n",
       "      <td>1205.297144</td>\n",
       "      <td>2450.076771</td>\n",
       "      <td>661.506672</td>\n",
       "      <td>Not-Fraud</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.439394</td>\n",
       "      <td>1.530303</td>\n",
       "      <td>3.674242</td>\n",
       "      <td>2.424242</td>\n",
       "      <td>71.371212</td>\n",
       "      <td>0.992424</td>\n",
       "      <td>4588.409091</td>\n",
       "      <td>502.166667</td>\n",
       "      <td>11.818182</td>\n",
       "      <td>11.871212</td>\n",
       "      <td>...</td>\n",
       "      <td>676.313985</td>\n",
       "      <td>4017.871066</td>\n",
       "      <td>260.257069</td>\n",
       "      <td>0.726572</td>\n",
       "      <td>0.653285</td>\n",
       "      <td>12620.604410</td>\n",
       "      <td>1226.306633</td>\n",
       "      <td>3369.338617</td>\n",
       "      <td>848.213675</td>\n",
       "      <td>Fraud</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.818792</td>\n",
       "      <td>1.604027</td>\n",
       "      <td>1.429530</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>73.516779</td>\n",
       "      <td>0.993289</td>\n",
       "      <td>350.134228</td>\n",
       "      <td>2.080537</td>\n",
       "      <td>11.865772</td>\n",
       "      <td>11.959732</td>\n",
       "      <td>...</td>\n",
       "      <td>694.246881</td>\n",
       "      <td>1536.290845</td>\n",
       "      <td>113.086257</td>\n",
       "      <td>0.667719</td>\n",
       "      <td>0.577420</td>\n",
       "      <td>11016.516940</td>\n",
       "      <td>1111.592405</td>\n",
       "      <td>2972.377916</td>\n",
       "      <td>808.138208</td>\n",
       "      <td>Not-Fraud</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.731330</td>\n",
       "      <td>1.599142</td>\n",
       "      <td>1.088412</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>71.783691</td>\n",
       "      <td>0.996567</td>\n",
       "      <td>241.124463</td>\n",
       "      <td>3.175966</td>\n",
       "      <td>11.907296</td>\n",
       "      <td>11.939914</td>\n",
       "      <td>...</td>\n",
       "      <td>630.805985</td>\n",
       "      <td>1234.005090</td>\n",
       "      <td>91.141252</td>\n",
       "      <td>0.657071</td>\n",
       "      <td>0.565930</td>\n",
       "      <td>10021.329570</td>\n",
       "      <td>957.701391</td>\n",
       "      <td>2727.944083</td>\n",
       "      <td>737.419878</td>\n",
       "      <td>Fraud</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.736111</td>\n",
       "      <td>1.527778</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>70.583333</td>\n",
       "      <td>0.986111</td>\n",
       "      <td>468.194444</td>\n",
       "      <td>45.333333</td>\n",
       "      <td>11.833333</td>\n",
       "      <td>11.833333</td>\n",
       "      <td>...</td>\n",
       "      <td>606.550334</td>\n",
       "      <td>1519.425993</td>\n",
       "      <td>103.302166</td>\n",
       "      <td>0.626542</td>\n",
       "      <td>0.520122</td>\n",
       "      <td>10565.761430</td>\n",
       "      <td>1126.358206</td>\n",
       "      <td>2486.827069</td>\n",
       "      <td>682.279276</td>\n",
       "      <td>Not-Fraud</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>2.548387</td>\n",
       "      <td>1.548387</td>\n",
       "      <td>2.806452</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>75.677419</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>127.741935</td>\n",
       "      <td>1.612903</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>539.926413</td>\n",
       "      <td>1278.578369</td>\n",
       "      <td>105.316369</td>\n",
       "      <td>0.642724</td>\n",
       "      <td>0.506470</td>\n",
       "      <td>10069.067870</td>\n",
       "      <td>1048.496358</td>\n",
       "      <td>2331.087492</td>\n",
       "      <td>676.226785</td>\n",
       "      <td>Not-Fraud</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1005</th>\n",
       "      <td>2.500000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>544.784235</td>\n",
       "      <td>589.615472</td>\n",
       "      <td>61.510304</td>\n",
       "      <td>0.690826</td>\n",
       "      <td>0.473344</td>\n",
       "      <td>10885.075840</td>\n",
       "      <td>1026.840019</td>\n",
       "      <td>2547.341333</td>\n",
       "      <td>723.822292</td>\n",
       "      <td>Not-Fraud</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1006</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>613.157895</td>\n",
       "      <td>4778.012673</td>\n",
       "      <td>463.684066</td>\n",
       "      <td>0.600751</td>\n",
       "      <td>0.710275</td>\n",
       "      <td>13241.321690</td>\n",
       "      <td>1469.095843</td>\n",
       "      <td>3203.267596</td>\n",
       "      <td>911.406530</td>\n",
       "      <td>Not-Fraud</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1007</th>\n",
       "      <td>2.333333</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>2.266667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>73.933333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>296.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>566.953462</td>\n",
       "      <td>2506.463260</td>\n",
       "      <td>131.832995</td>\n",
       "      <td>0.726815</td>\n",
       "      <td>0.354062</td>\n",
       "      <td>11262.841610</td>\n",
       "      <td>1196.045563</td>\n",
       "      <td>2691.729344</td>\n",
       "      <td>736.415563</td>\n",
       "      <td>Not-Fraud</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1008</th>\n",
       "      <td>2.634921</td>\n",
       "      <td>1.539683</td>\n",
       "      <td>1.349206</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>77.873016</td>\n",
       "      <td>0.984127</td>\n",
       "      <td>204.444444</td>\n",
       "      <td>0.793651</td>\n",
       "      <td>11.809524</td>\n",
       "      <td>11.809524</td>\n",
       "      <td>...</td>\n",
       "      <td>587.854714</td>\n",
       "      <td>1114.462613</td>\n",
       "      <td>97.400585</td>\n",
       "      <td>0.599785</td>\n",
       "      <td>0.533154</td>\n",
       "      <td>9713.456656</td>\n",
       "      <td>1014.932899</td>\n",
       "      <td>2579.061940</td>\n",
       "      <td>707.121804</td>\n",
       "      <td>Not-Fraud</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1009 rows Ã— 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      phy_same  phy_count    period  periodadmt        age     alife  \\\n",
       "0     2.960000   1.600000  1.440000    1.000000  80.240000  1.000000   \n",
       "1     2.439394   1.530303  3.674242    2.424242  71.371212  0.992424   \n",
       "2     2.818792   1.604027  1.429530    0.000000  73.516779  0.993289   \n",
       "3     2.731330   1.599142  1.088412    0.000000  71.783691  0.996567   \n",
       "4     2.736111   1.527778  0.958333    0.222222  70.583333  0.986111   \n",
       "...        ...        ...       ...         ...        ...       ...   \n",
       "1004  2.548387   1.548387  2.806452    0.000000  75.677419  1.000000   \n",
       "1005  2.500000   1.500000  0.100000    0.000000  78.000000  1.000000   \n",
       "1006  3.000000   2.000000  0.000000    0.000000  74.000000  1.000000   \n",
       "1007  2.333333   1.333333  2.266667    0.000000  73.933333  1.000000   \n",
       "1008  2.634921   1.539683  1.349206    0.000000  77.873016  0.984127   \n",
       "\n",
       "      provider_InscClaimAmtReimbursed_mean  provider_DeductibleAmtPaid_mean  \\\n",
       "0                              4185.600000                       213.600000   \n",
       "1                              4588.409091                       502.166667   \n",
       "2                               350.134228                         2.080537   \n",
       "3                               241.124463                         3.175966   \n",
       "4                               468.194444                        45.333333   \n",
       "...                                    ...                              ...   \n",
       "1004                            127.741935                         1.612903   \n",
       "1005                             72.000000                         0.000000   \n",
       "1006                             50.000000                         0.000000   \n",
       "1007                            296.000000                         0.000000   \n",
       "1008                            204.444444                         0.793651   \n",
       "\n",
       "      provider_NoOfMonths_PartACov_mean  provider_NoOfMonths_PartBCov_mean  \\\n",
       "0                             12.000000                          12.000000   \n",
       "1                             11.818182                          11.871212   \n",
       "2                             11.865772                          11.959732   \n",
       "3                             11.907296                          11.939914   \n",
       "4                             11.833333                          11.833333   \n",
       "...                                 ...                                ...   \n",
       "1004                          12.000000                          12.000000   \n",
       "1005                          12.000000                          12.000000   \n",
       "1006                          12.000000                          12.000000   \n",
       "1007                          12.000000                          12.000000   \n",
       "1008                          11.809524                          11.809524   \n",
       "\n",
       "      ...  diag1_OPAnnualDeductibleAmt_mean  diag1_InscClaimAmtReimbursed_std  \\\n",
       "0     ...                        543.045084                       3482.066310   \n",
       "1     ...                        676.313985                       4017.871066   \n",
       "2     ...                        694.246881                       1536.290845   \n",
       "3     ...                        630.805985                       1234.005090   \n",
       "4     ...                        606.550334                       1519.425993   \n",
       "...   ...                               ...                               ...   \n",
       "1004  ...                        539.926413                       1278.578369   \n",
       "1005  ...                        544.784235                        589.615472   \n",
       "1006  ...                        613.157895                       4778.012673   \n",
       "1007  ...                        566.953462                       2506.463260   \n",
       "1008  ...                        587.854714                       1114.462613   \n",
       "\n",
       "      diag1_DeductibleAmtPaid_std  diag1_NoOfMonths_PartACov_std  \\\n",
       "0                      161.353027                       0.569945   \n",
       "1                      260.257069                       0.726572   \n",
       "2                      113.086257                       0.667719   \n",
       "3                       91.141252                       0.657071   \n",
       "4                      103.302166                       0.626542   \n",
       "...                           ...                            ...   \n",
       "1004                   105.316369                       0.642724   \n",
       "1005                    61.510304                       0.690826   \n",
       "1006                   463.684066                       0.600751   \n",
       "1007                   131.832995                       0.726815   \n",
       "1008                    97.400585                       0.599785   \n",
       "\n",
       "      diag1_NoOfMonths_PartBCov_std  diag1_IPAnnualReimbursementAmt_std  \\\n",
       "0                          0.424192                        12941.552350   \n",
       "1                          0.653285                        12620.604410   \n",
       "2                          0.577420                        11016.516940   \n",
       "3                          0.565930                        10021.329570   \n",
       "4                          0.520122                        10565.761430   \n",
       "...                             ...                                 ...   \n",
       "1004                       0.506470                        10069.067870   \n",
       "1005                       0.473344                        10885.075840   \n",
       "1006                       0.710275                        13241.321690   \n",
       "1007                       0.354062                        11262.841610   \n",
       "1008                       0.533154                         9713.456656   \n",
       "\n",
       "      diag1_IPAnnualDeductibleAmt_std  diag1_OPAnnualReimbursementAmt_std  \\\n",
       "0                         1205.297144                         2450.076771   \n",
       "1                         1226.306633                         3369.338617   \n",
       "2                         1111.592405                         2972.377916   \n",
       "3                          957.701391                         2727.944083   \n",
       "4                         1126.358206                         2486.827069   \n",
       "...                               ...                                 ...   \n",
       "1004                      1048.496358                         2331.087492   \n",
       "1005                      1026.840019                         2547.341333   \n",
       "1006                      1469.095843                         3203.267596   \n",
       "1007                      1196.045563                         2691.729344   \n",
       "1008                      1014.932899                         2579.061940   \n",
       "\n",
       "      diag1_OPAnnualDeductibleAmt_std  PotentialFraud  \n",
       "0                          661.506672       Not-Fraud  \n",
       "1                          848.213675           Fraud  \n",
       "2                          808.138208       Not-Fraud  \n",
       "3                          737.419878           Fraud  \n",
       "4                          682.279276       Not-Fraud  \n",
       "...                               ...             ...  \n",
       "1004                       676.226785       Not-Fraud  \n",
       "1005                       723.822292       Not-Fraud  \n",
       "1006                       911.406530       Not-Fraud  \n",
       "1007                       736.415563       Not-Fraud  \n",
       "1008                       707.121804       Not-Fraud  \n",
       "\n",
       "[1009 rows x 49 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path_fraud = './data/fraud_data_m_oct_ready.csv'\n",
    "fraud_data = pd.read_csv(data_path_fraud)\n",
    "fraud_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = fraud_data.iloc[:, 0:47].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = fraud_data[\"PotentialFraud\"].apply(lambda val: 0 if val == \"Not-Fraud\" else 1).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 93.56%\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=7)\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]    \n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thresh=0.000, n=47, Accuracy: 93.56%\n",
      "Thresh=0.000, n=47, Accuracy: 93.56%\n",
      "Thresh=0.000, n=47, Accuracy: 93.56%\n",
      "Thresh=0.000, n=47, Accuracy: 93.56%\n",
      "Thresh=0.000, n=47, Accuracy: 93.56%\n",
      "Thresh=0.000, n=47, Accuracy: 93.56%\n",
      "Thresh=0.005, n=41, Accuracy: 93.56%\n",
      "Thresh=0.009, n=40, Accuracy: 94.06%\n",
      "Thresh=0.009, n=39, Accuracy: 94.06%\n",
      "Thresh=0.009, n=38, Accuracy: 94.06%\n",
      "Thresh=0.010, n=37, Accuracy: 94.06%\n",
      "Thresh=0.011, n=36, Accuracy: 94.55%\n",
      "Thresh=0.011, n=35, Accuracy: 94.06%\n",
      "Thresh=0.011, n=34, Accuracy: 94.06%\n",
      "Thresh=0.012, n=33, Accuracy: 95.05%\n",
      "Thresh=0.012, n=32, Accuracy: 94.06%\n",
      "Thresh=0.012, n=31, Accuracy: 95.54%\n",
      "Thresh=0.012, n=30, Accuracy: 94.06%\n",
      "Thresh=0.013, n=29, Accuracy: 95.05%\n",
      "Thresh=0.013, n=28, Accuracy: 93.56%\n",
      "Thresh=0.014, n=27, Accuracy: 94.06%\n",
      "Thresh=0.014, n=26, Accuracy: 94.06%\n",
      "Thresh=0.014, n=25, Accuracy: 94.06%\n",
      "Thresh=0.015, n=24, Accuracy: 95.54%\n",
      "Thresh=0.015, n=23, Accuracy: 94.55%\n",
      "Thresh=0.016, n=22, Accuracy: 93.56%\n",
      "Thresh=0.017, n=21, Accuracy: 94.06%\n",
      "Thresh=0.017, n=20, Accuracy: 94.55%\n",
      "Thresh=0.020, n=19, Accuracy: 94.06%\n",
      "Thresh=0.020, n=18, Accuracy: 93.56%\n",
      "Thresh=0.020, n=17, Accuracy: 94.55%\n",
      "Thresh=0.022, n=16, Accuracy: 93.56%\n",
      "Thresh=0.023, n=15, Accuracy: 93.56%\n",
      "Thresh=0.024, n=14, Accuracy: 93.07%\n",
      "Thresh=0.025, n=13, Accuracy: 93.07%\n",
      "Thresh=0.025, n=12, Accuracy: 92.57%\n",
      "Thresh=0.026, n=11, Accuracy: 94.55%\n",
      "Thresh=0.026, n=10, Accuracy: 94.06%\n",
      "Thresh=0.029, n=9, Accuracy: 94.06%\n",
      "Thresh=0.030, n=8, Accuracy: 94.55%\n",
      "Thresh=0.032, n=7, Accuracy: 95.05%\n",
      "Thresh=0.045, n=6, Accuracy: 93.56%\n",
      "Thresh=0.046, n=5, Accuracy: 94.06%\n",
      "Thresh=0.047, n=4, Accuracy: 93.07%\n",
      "Thresh=0.063, n=3, Accuracy: 93.07%\n",
      "Thresh=0.080, n=2, Accuracy: 91.58%\n",
      "Thresh=0.127, n=1, Accuracy: 90.59%\n"
     ]
    }
   ],
   "source": [
    "thresholds = sort(model.feature_importances_)\n",
    "max_acc = -1\n",
    "true_thresh = -1\n",
    "for thresh in thresholds:\n",
    "    selection = SelectFromModel(model, threshold=thresh, prefit=True)\n",
    "    select_X_train = selection.transform(X_train)\n",
    "    \n",
    "    selection_model = XGBClassifier()\n",
    "    selection_model.fit(select_X_train, y_train)\n",
    "    \n",
    "    select_X_test = selection.transform(X_test)\n",
    "    y_pred = selection_model.predict(select_X_test)\n",
    "    \n",
    "    predictions = [round(value) for value in y_pred]\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    if accuracy >= max_acc and select_X_train.shape[1] < 20:\n",
    "        max_acc = accuracy\n",
    "        true_thresh = thresh\n",
    "    print(\"Thresh=%.3f, n=%d, Accuracy: %.2f%%\" % (thresh, select_X_train.shape[1], accuracy*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.032147862\n"
     ]
    }
   ],
   "source": [
    "print(true_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "from lsopt.tree import OptimalTreeClassifier ## M-OCT propsed by Liu & Allen\n",
    "from lsopt.tree import BinNodePenaltyOptimalTreeClassifier ## BNP-OCT propsed by Liu & Allen\n",
    "selection = SelectFromModel(model, threshold=true_thresh, prefit=True)\n",
    "select_X_train = selection.transform(X_train)\n",
    "\n",
    "# OCT parameters\n",
    "max_depth = 3\n",
    "min_samples_leaf = 10\n",
    "alpha = 0.00005\n",
    "time_limit = 15 # minute\n",
    "mip_gap_tol = 0.01  # optimal gap percentage\n",
    "mip_focus = 'balance'\n",
    "mip_polish_time = None\n",
    "warm_start = False\n",
    "log_file = None\n",
    "fp_heur = True\n",
    "backtrack = \"bestb\"\n",
    "\n",
    "# Construct BNP-OCT classifier\n",
    "selection_model = BinNodePenaltyOptimalTreeClassifier(max_depth=max_depth,min_samples_leaf=min_samples_leaf,\n",
    "                                  alpha=alpha,\n",
    "                                  criterion=\"gini\",\n",
    "                                  solver=\"gurobi\",\n",
    "                                  time_limit=time_limit,\n",
    "                                  verbose=True,\n",
    "                                  warm_start=warm_start,\n",
    "                                  log_file=log_file,\n",
    "                                  solver_options={'mip_cuts': 'auto',\n",
    "                                                  'mip_gap_tol': mip_gap_tol,\n",
    "                                                  'mip_focus': mip_focus,\n",
    "                                                  'mip_polish_time': mip_polish_time,\n",
    "                                                  }\n",
    "                                  )\n",
    "\n",
    "names = []\n",
    "for i in range(len(model.feature_importances_)):\n",
    "    if model.feature_importances_[i] >= true_thresh:\n",
    "        names.append(fraud_data.iloc[:, [i]].columns[0])\n",
    "print(len(names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation of below code\n",
    "Alright, there's a few functions here, so lets go through them one by one.\n",
    "\n",
    "### Split\n",
    "This one is pretty straitforward, its the one that actually splits the data into a training set and a testing set\n",
    "\n",
    "### acc_score\n",
    "Returns the accuracy of each model\n",
    "\n",
    "### plot\n",
    "For plotting any results of the model, if I need to visualize it\n",
    "\n",
    "### initialization_of_population\n",
    "To initialize a random population.\n",
    "\n",
    "### fitness_score\n",
    "Returns the best parents for the next run, along with their fitness scores\n",
    "\n",
    "### selection\n",
    "Selects the best parents at the end of every run\n",
    "\n",
    "### crossover\n",
    "Picks half of the first parents, and half of the second\n",
    "\n",
    "### mutation\n",
    "Randomly flip the a feature from True to False (holding true to the principle of the genetic algorithm)\n",
    "\n",
    "### generations\n",
    "Executes all the above functions for the specified number of generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(df,label):\n",
    "    selection = SelectFromModel(model, threshold=true_thresh, prefit=True)\n",
    "    X_tr = select_X_train\n",
    "    X_te = X_test\n",
    "    Y_tr = y_train\n",
    "    Y_te = y_test\n",
    "    return X_tr, X_te, Y_tr, Y_te\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from xgboost import XGBClassifier\n",
    "import random\n",
    "\n",
    "classifiers = ['LinearSVM', 'RadialSVM', \n",
    "               'Logistic',  'RandomForest', \n",
    "               'AdaBoost',  'DecisionTree', \n",
    "               'KNeighbors','GradientBoosting', 'BNP-OCT']\n",
    "\n",
    "models = [svm.SVC(kernel='linear'),\n",
    "          svm.SVC(kernel='rbf'),\n",
    "          LogisticRegression(max_iter = 1000),\n",
    "          RandomForestClassifier(n_estimators=200, random_state=0),\n",
    "          AdaBoostClassifier(random_state = 0),\n",
    "          DecisionTreeClassifier(random_state=0),\n",
    "          KNeighborsClassifier(),\n",
    "          XGBClassifier(random_state=0),\n",
    "          BinNodePenaltyOptimalTreeClassifier(max_depth=max_depth,min_samples_leaf=min_samples_leaf,\n",
    "                                  alpha=alpha,\n",
    "                                  criterion=\"gini\",\n",
    "                                  solver=\"gurobi\",\n",
    "                                  time_limit=time_limit,\n",
    "                                  verbose=True,\n",
    "                                  warm_start=warm_start,\n",
    "                                  log_file=log_file,\n",
    "                                  solver_options={'mip_cuts': None,\n",
    "                                                  'mip_gap_tol': mip_gap_tol,\n",
    "                                                  'mip_focus': mip_focus,\n",
    "                                                  'mip_polish_time': mip_polish_time,\n",
    "                                                  }\n",
    "                                  )]\n",
    "\n",
    "\n",
    "def acc_score(df,label):\n",
    "    Score = pd.DataFrame({\"Classifier\":classifiers})\n",
    "    j = 0\n",
    "    acc = []\n",
    "    X_train = select_X_train\n",
    "    X_test = select_X_train\n",
    "    Y_train = y_train\n",
    "    Y_test = y_train\n",
    "    for i in models:\n",
    "        model = i\n",
    "        model.fit(X_train,Y_train)\n",
    "        predictions = model.predict(X_test)\n",
    "        acc.append(accuracy_score(Y_test,predictions))\n",
    "        j = j+1     \n",
    "    Score[\"Accuracy\"] = acc\n",
    "    Score.sort_values(by=\"Accuracy\", ascending=False,inplace = True)\n",
    "    Score.reset_index(drop=True, inplace=True)\n",
    "    return Score\n",
    "\n",
    "def plot(score,x,y,c = \"b\"):\n",
    "    gen = [1,2,3,4,5]\n",
    "    plt.figure(figsize=(6,4))\n",
    "    ax = sns.pointplot(x=gen, y=score,color = c )\n",
    "    ax.set(xlabel=\"Generation\", ylabel=\"Accuracy\")\n",
    "    ax.set(ylim=(x,y))\n",
    "\n",
    "# Initialization of population\n",
    "def initilization_of_population(size):\n",
    "    population = [{\"min_samples_leaf\": 10, \"mip_gap_tol\": 0.01, \"alpha\": 0.00005}, \n",
    "                  {\"min_samples_leaf\": 7, \"mip_gap_tol\": 0.05, \"alpha\": 0.00003}, \n",
    "                  {\"min_samples_leaf\": 5, \"mip_gap_tol\": 0.04, \"alpha\": 0.0001}, \n",
    "                  {\"min_samples_leaf\": 9, \"mip_gap_tol\": 0.1, \"alpha\": 0.00008}]\n",
    "    return population\n",
    "\n",
    "\n",
    "def fitness_score(population):\n",
    "    scores = []\n",
    "    for chromosome in population:\n",
    "        model = BinNodePenaltyOptimalTreeClassifier(max_depth=max_depth,min_samples_leaf=chromosome[\"min_samples_leaf\"],\n",
    "                                  alpha=chromosome[\"alpha\"],\n",
    "                                  criterion=\"gini\",\n",
    "                                  solver=\"gurobi\",\n",
    "                                  time_limit=time_limit,\n",
    "                                  verbose=True,\n",
    "                                  warm_start=warm_start,\n",
    "                                  log_file=log_file,\n",
    "                                  solver_options={'mip_cuts': 'auto',\n",
    "                                                  'mip_gap_tol': chromosome[\"mip_gap_tol\"],\n",
    "                                                  'mip_focus': mip_focus,\n",
    "                                                  'mip_polish_time': mip_polish_time,\n",
    "                                                  }\n",
    "                                  )\n",
    "        model.fit(select_X_train, y_train)\n",
    "        predictions = model.predict(select_X_train)\n",
    "        scores.append(accuracy_score(y_train, predictions))\n",
    "    scores, population = np.array(scores), np.array(population)\n",
    "    inds = np.argsort(scores)\n",
    "    return list(scores[inds][::-1]), list(population[inds][::-1])\n",
    "        \n",
    "    # scores = []\n",
    "    # for chromosome in population:\n",
    "    #     logmodel.fit(select_X_train[:, chromosome],y_train)         \n",
    "    #     predictions = logmodel.predict(select_X_train[:, chromosome])\n",
    "    #     scores.append(accuracy_score(y_train,predictions))\n",
    "    # scores, population = np.array(scores), np.array(population) \n",
    "    # inds = np.argsort(scores)                                    \n",
    "    # return list(scores[inds][::-1]), list(population[inds,:][::-1]) \n",
    "\n",
    "def selection(pop_after_fit,n_parents):\n",
    "    population_nextgen = []\n",
    "    for i in range(n_parents):\n",
    "        population_nextgen.append(pop_after_fit[i])\n",
    "    return population_nextgen\n",
    "\n",
    "\n",
    "def crossover(pop_after_sel):\n",
    "    pop_nextgen = pop_after_sel\n",
    "    for i in range(0,len(pop_after_sel),2):\n",
    "        new_par = {}\n",
    "        child_1, child_2 = pop_nextgen[i] , pop_nextgen[i+1]\n",
    "        num = random.uniform(0, 1)\n",
    "        if (num > 0.5):\n",
    "            new_par[\"min_samples_leaf\"] = child_1[\"min_samples_leaf\"]\n",
    "            new_par[\"mip_gap_tol\"] = child_1[\"mip_gap_tol\"]\n",
    "            new_par[\"alpha\"] = child_2[\"alpha\"]\n",
    "        else:\n",
    "            new_par[\"min_samples_leaf\"] = child_2[\"min_samples_leaf\"]\n",
    "            new_par[\"mip_gap_tol\"] = child_2[\"mip_gap_tol\"]\n",
    "            new_par[\"alpha\"] = child_1[\"alpha\"]\n",
    "        pop_nextgen.append(new_par)\n",
    "    return pop_nextgen\n",
    "    # pop_nextgen = pop_after_sel\n",
    "    # for i in range(0,len(pop_after_sel),2):\n",
    "    #     new_par = []\n",
    "    #     child_1, child_2 = pop_nextgen[i] , pop_nextgen[i+1]\n",
    "    #     new_par = np.concatenate((child_1[:len(child_1)//2],child_2[len(child_1)//2:]))\n",
    "    #     pop_nextgen.append(new_par)\n",
    "    # return pop_nextgen\n",
    "\n",
    "def mutation(pop_after_cross,mutation_rate):   \n",
    "    mutation_range = int(mutation_rate*3)\n",
    "    pop_next_gen = []\n",
    "    for n in range(len(pop_after_cross)):\n",
    "        chromo = pop_after_cross[n]\n",
    "        # rand_num = random.randint(0, 2)\n",
    "        # if rand_num == 0:\n",
    "        #     chromo\n",
    "        pop_next_gen.append(chromo)\n",
    "    return pop_next_gen\n",
    "\n",
    "def generations(df,label,size,n_feat,n_parents,mutation_rate,n_gen,X_train,\n",
    "                                   X_test, Y_train, Y_test):\n",
    "    best_chromo= []\n",
    "    best_score= []\n",
    "    population_nextgen=initilization_of_population(size)\n",
    "    for i in range(n_gen):\n",
    "        scores, pop_after_fit = fitness_score(population_nextgen)\n",
    "        print(\"pop_after_fit\", pop_after_fit)\n",
    "        print('Best score in generation',i+1,':',scores[:1])  #2\n",
    "        pop_after_sel = selection(pop_after_fit,n_parents)\n",
    "        pop_after_cross = crossover(pop_after_sel)\n",
    "        population_nextgen = mutation(pop_after_cross,mutation_rate)\n",
    "        best_chromo.append(pop_after_fit[0])\n",
    "        best_score.append(scores[0])\n",
    "    return best_chromo,best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation of cell below\n",
    "The below cell essentially generates the accuracy of all the possible model classifiers that can be used (it appears that XGBoost is best, along with AdaBoost). Basically, what we're doing is running each model on the 7 features pre-selected with XGBoost, to determine which one runs in the most accurarte fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set parameter WLSAccessID\n",
      "Set parameter WLSSecret\n",
      "Set parameter LicenseID to value 2433923\n",
      "WLS license - registered to pranav.chaudhary@sagataltd.io\n",
      "Read LP format model from file /var/folders/kb/sxk47lqj0xd8j0x92mykb0rr0000gn/T/tmpu3fhy2vm.pyomo.lp\n",
      "Reading time = 0.19 seconds\n",
      "x1: 26210 rows, 6622 columns, 438053 nonzeros\n",
      "Set parameter TimeLimit to value 900\n",
      "Set parameter MIPGap to value 0.01\n",
      "Gurobi Optimizer version 10.0.3 build v10.0.3rc0 (mac64[x86])\n",
      "\n",
      "CPU model: Intel(R) Core(TM) i5-1038NG7 CPU @ 2.00GHz\n",
      "Thread count: 4 physical cores, 8 logical processors, using up to 8 threads\n",
      "\n",
      "WLS license - registered to pranav.chaudhary@sagataltd.io\n",
      "Optimize a model with 26210 rows, 6622 columns and 438053 nonzeros\n",
      "Model fingerprint: 0x9ef9563b\n",
      "Variable types: 6489 continuous, 133 integer (133 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+03]\n",
      "  Objective range  [5e-05, 1e-02]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 1e+03]\n",
      "Presolve removed 2026 rows and 451 columns\n",
      "Presolve time: 1.66s\n",
      "Presolved: 24184 rows, 6171 columns, 422661 nonzeros\n",
      "Variable types: 6044 continuous, 127 integer (127 binary)\n",
      "Found heuristic solution: objective 0.7981028\n",
      "\n",
      "Root simplex log...\n",
      "\n",
      "Iteration    Objective       Primal Inf.    Dual Inf.      Time\n",
      "    3087    9.8244490e-04   2.494595e+04   0.000000e+00      5s\n",
      "    5029    5.0000000e-05   0.000000e+00   0.000000e+00      8s\n",
      "\n",
      "Root relaxation: objective 5.000000e-05, 5029 iterations, 5.40 seconds (10.66 work units)\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "     0     0    0.00005    0    5    0.79810    0.00005   100%     -    7s\n",
      "H    0     0                       0.7531590    0.00005   100%     -    8s\n",
      "     0     0    0.00005    0    5    0.75316    0.00005   100%     -   37s\n",
      "     0     0    0.00005    0    5    0.75316    0.00005   100%     -   39s\n",
      "     0     0    0.00005    0    6    0.75316    0.00005   100%     -   68s\n",
      "H    0     0                       0.7531090    0.00005   100%     -   69s\n",
      "     0     0    0.00005    0    5    0.75311    0.00005   100%     -   74s\n",
      "     0     0    0.00005    0    7    0.75311    0.00005   100%     -   99s\n",
      "     0     0    0.00005    0    7    0.75311    0.00005   100%     -  105s\n",
      "     0     0    0.00005    0    5    0.75311    0.00005   100%     -  124s\n",
      "     0     0    0.00005    0    7    0.75311    0.00005   100%     -  156s\n",
      "     0     0    0.00005    0    5    0.75311    0.00005   100%     -  188s\n",
      "     0     0    0.00005    0    5    0.75311    0.00005   100%     -  188s\n",
      "     0     2    0.00005    0    5    0.75311    0.00005   100%     -  241s\n",
      "     1     4    0.00005    1    7    0.75311    0.00005   100%  7418  297s\n",
      "     3     8    0.00010    2    9    0.75311    0.00005   100% 20470  329s\n",
      "     7    12    0.00010    3   14    0.75311    0.00005   100% 19426  406s\n",
      "    11    16    0.00010    3   11    0.75311    0.00007   100% 14477  430s\n",
      "    15    20    0.00010    4   15    0.75311    0.00007   100% 13100  480s\n",
      "    19    24    0.00010    5    9    0.75311    0.00007   100% 10930  493s\n",
      "    23    31    0.00010    6    7    0.75311    0.00007   100% 10232  506s\n",
      "    30    37    0.00010    7    7    0.75311    0.00007   100%  8653  515s\n",
      "    36    43    0.00011    7   28    0.75311    0.00007   100%  8035  533s\n",
      "    42    50    0.00020    8   18    0.75311    0.00007   100%  7377  540s\n",
      "    49    60    0.00011    8    8    0.75311    0.00007   100%  6743  548s\n",
      "    59    71    0.00015    9    7    0.75311    0.00007   100%  5818  556s\n",
      "    70    75    0.00015   11    7    0.75311    0.00007   100%  5192  563s\n",
      "    82    86    0.00015   12    7    0.75311    0.00007   100%  4563  574s\n",
      "    95   100    0.00025   14   19    0.75311    0.00007   100%  4201  597s\n",
      "   109   107    0.00015   16    8    0.75311    0.00007   100%  3967  626s\n",
      "   124   122 infeasible   18         0.75311    0.00007   100%  3854  650s\n",
      "   141   141    0.05771   24   19    0.75311    0.00007   100%  3685  664s\n",
      "   160   164    0.13513   30   18    0.75311    0.00007   100%  3482  672s\n",
      "   187   184    0.13515   37   22    0.75311    0.00007   100%  3118  682s\n",
      "   211   216    0.13518   44   20    0.75311    0.00007   100%  2921  691s\n",
      "   244   239 infeasible   51         0.75311    0.00007   100%  2616  698s\n",
      "H  257   239                       0.7418730    0.00007   100%  2485  698s\n",
      "   280   253     cutoff   59         0.74187    0.00010   100%  2368  726s\n",
      "   315   286 infeasible   10         0.74187    0.00010   100%  2249  737s\n",
      "   364   302    0.02092    9    7    0.74187    0.00010   100%  2029  793s\n",
      "   413   312    0.00016    7   26    0.74187    0.00010   100%  2005  820s\n",
      "H  436   310                       0.7417730    0.00010   100%  1958  820s\n",
      "   439   332    0.00025    8   21    0.74177    0.00010   100%  1970  837s\n",
      "   467   354    0.18003   10   16    0.74177    0.00010   100%  1934  862s\n",
      "H  494   305                       0.7080652    0.00010   100%  1908  862s\n",
      "   495   316    0.52844   30   12    0.70807    0.00010   100%  1904  897s\n",
      "   545   317    0.00016    6   30    0.70807    0.00010   100%  1840  900s\n",
      "\n",
      "Cutting planes:\n",
      "  Implied bound: 421\n",
      "  MIR: 240\n",
      "  Flow cover: 25\n",
      "  RLT: 15\n",
      "  Relax-and-lift: 410\n",
      "\n",
      "Explored 554 nodes (1082008 simplex iterations) in 900.04 seconds (1874.43 work units)\n",
      "Thread count was 8 (of 8 available processors)\n",
      "\n",
      "Solution count 6: 0.708065 0.741773 0.741873 ... 0.798103\n",
      "\n",
      "Time limit reached\n",
      "Best objective 7.080651685393e-01, best bound 1.000000000000e-04, gap 99.9859%\n",
      "WARNING: Loading a SolverResults object with an 'aborted' status, but\n",
      "containing a solution\n",
      "Solver running time: 901.708820104599\n",
      "Solver termination condition: maxTimeLimit\n",
      "Valid Tree : Yes\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DecisionTree</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>0.998761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.944238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BNP-OCT</td>\n",
       "      <td>0.921933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>KNeighbors</td>\n",
       "      <td>0.909542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RadialSVM</td>\n",
       "      <td>0.895911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Logistic</td>\n",
       "      <td>0.887237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LinearSVM</td>\n",
       "      <td>0.881041</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Classifier  Accuracy\n",
       "0      RandomForest  1.000000\n",
       "1      DecisionTree  1.000000\n",
       "2  GradientBoosting  0.998761\n",
       "3          AdaBoost  0.944238\n",
       "4           BNP-OCT  0.921933\n",
       "5        KNeighbors  0.909542\n",
       "6         RadialSVM  0.895911\n",
       "7          Logistic  0.887237\n",
       "8         LinearSVM  0.881041"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = acc_score(select_X_train, y_train)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation Of Below Cell\n",
    "\n",
    "Below, I'm running the GA with BNP-OCT as its base. Basically, using the 7 features already selected by XGBoost, the idea is the GA is being used for further feature selection here. Basically, what its doing is running over multiple possible feature combinations with BNP-OCT as the classifier that its exploring. The algorithm is selecting the most accurate BNP-OCT outputs with that, and continuing to recombine, mutate etc. Currently, its taking very long, because each generation has about 2 possible solutions being explored, and those are recombined for the next set, with each possible model taking 10 minutes to train, so all combination can take upto and 1 to an hour and half to run. Also, there's a posssibility that some solutions are invalid, so if we hit upon those in the GA, it means it came across an invalid solution. Currently, I'm fixing that by removing the mutation rate, and also trying to increase the set of explored parent solution beforem moving generations (as the greater the number of parent solutions, the greater chance that the GA will ignore some in the next generation, only selecting the correct ones, but its a balancing act of not having too many parents that training takes an absurd amount of time. With XGBoost, the number of required parents was 64, which is too large a number of BNP-OCT to explore with GA).\n",
    "\n",
    "## Next steps\n",
    "If this ends up not producing anything reasonable, I could try exploring GA on hyperparameters (but that would be a lot longer on implementation, as the GA would have to be altered in a fundamental way). \n",
    "\n",
    "Finally, we should check with Ted, if he wants GA to be used as an alternative to MILP, as that would probably be the multi-month project of delving into _base.py and tree.py and altering those algos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set parameter WLSAccessID\n",
      "Set parameter WLSSecret\n",
      "Set parameter LicenseID to value 2433923\n",
      "WLS license - registered to pranav.chaudhary@sagataltd.io\n",
      "Read LP format model from file /var/folders/kb/sxk47lqj0xd8j0x92mykb0rr0000gn/T/tmptkoiqrdw.pyomo.lp\n",
      "Reading time = 0.17 seconds\n",
      "x1: 26210 rows, 6622 columns, 438053 nonzeros\n",
      "Set parameter TimeLimit to value 900\n",
      "Set parameter MIPGap to value 0.01\n",
      "Gurobi Optimizer version 10.0.3 build v10.0.3rc0 (mac64[x86])\n",
      "\n",
      "CPU model: Intel(R) Core(TM) i5-1038NG7 CPU @ 2.00GHz\n",
      "Thread count: 4 physical cores, 8 logical processors, using up to 8 threads\n",
      "\n",
      "WLS license - registered to pranav.chaudhary@sagataltd.io\n",
      "Optimize a model with 26210 rows, 6622 columns and 438053 nonzeros\n",
      "Model fingerprint: 0x9ef9563b\n",
      "Variable types: 6489 continuous, 133 integer (133 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+03]\n",
      "  Objective range  [5e-05, 1e-02]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 1e+03]\n",
      "Presolve removed 2026 rows and 451 columns\n",
      "Presolve time: 1.39s\n",
      "Presolved: 24184 rows, 6171 columns, 422661 nonzeros\n",
      "Variable types: 6044 continuous, 127 integer (127 binary)\n",
      "Found heuristic solution: objective 0.7981028\n",
      "\n",
      "Root simplex log...\n",
      "\n",
      "Iteration    Objective       Primal Inf.    Dual Inf.      Time\n",
      "    3837    1.0061334e-03   3.318176e+04   0.000000e+00      5s\n",
      "    5029    5.0000000e-05   0.000000e+00   0.000000e+00      6s\n",
      "\n",
      "Root relaxation: objective 5.000000e-05, 5029 iterations, 4.75 seconds (10.66 work units)\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "     0     0    0.00005    0    5    0.79810    0.00005   100%     -    6s\n",
      "H    0     0                       0.7531590    0.00005   100%     -    6s\n",
      "     0     0    0.00005    0    5    0.75316    0.00005   100%     -   30s\n",
      "     0     0    0.00005    0    5    0.75316    0.00005   100%     -   32s\n",
      "     0     0    0.00005    0    6    0.75316    0.00005   100%     -   55s\n",
      "H    0     0                       0.7531090    0.00005   100%     -   56s\n",
      "     0     0    0.00005    0    5    0.75311    0.00005   100%     -   61s\n",
      "     0     0    0.00005    0    7    0.75311    0.00005   100%     -   81s\n",
      "     0     0    0.00005    0    7    0.75311    0.00005   100%     -   86s\n",
      "     0     0    0.00005    0    5    0.75311    0.00005   100%     -  101s\n",
      "     0     0    0.00005    0    7    0.75311    0.00005   100%     -  131s\n",
      "     0     0    0.00005    0    5    0.75311    0.00005   100%     -  161s\n",
      "     0     0    0.00005    0    5    0.75311    0.00005   100%     -  161s\n",
      "     0     2    0.00005    0    5    0.75311    0.00005   100%     -  218s\n",
      "     1     4    0.00005    1    7    0.75311    0.00005   100%  7418  271s\n",
      "     3     8    0.00010    2    9    0.75311    0.00005   100% 20470  303s\n",
      "     7    12    0.00010    3   14    0.75311    0.00005   100% 19426  378s\n",
      "    11    16    0.00010    3   11    0.75311    0.00007   100% 14477  401s\n",
      "    15    20    0.00010    4   15    0.75311    0.00007   100% 13100  443s\n",
      "    19    24    0.00010    5    9    0.75311    0.00007   100% 10930  454s\n",
      "    23    31    0.00010    6    7    0.75311    0.00007   100% 10232  465s\n",
      "    30    37    0.00010    7    7    0.75311    0.00007   100%  8653  472s\n",
      "    36    43    0.00011    7   28    0.75311    0.00007   100%  8035  486s\n",
      "    42    50    0.00020    8   18    0.75311    0.00007   100%  7377  490s\n",
      "    59    71    0.00015    9    7    0.75311    0.00007   100%  5818  498s\n",
      "    70    75    0.00015   11    7    0.75311    0.00007   100%  5192  502s\n",
      "    82    86    0.00015   12    7    0.75311    0.00007   100%  4563  513s\n",
      "    95   100    0.00025   14   19    0.75311    0.00007   100%  4201  531s\n",
      "   109   107    0.00015   16    8    0.75311    0.00007   100%  3967  554s\n",
      "   124   122 infeasible   18         0.75311    0.00007   100%  3854  574s\n",
      "   141   141    0.05771   24   19    0.75311    0.00007   100%  3685  583s\n",
      "   160   164    0.13513   30   18    0.75311    0.00007   100%  3482  591s\n",
      "   187   184    0.13515   37   22    0.75311    0.00007   100%  3118  600s\n",
      "   211   216    0.13518   44   20    0.75311    0.00007   100%  2921  606s\n",
      "   244   239 infeasible   51         0.75311    0.00007   100%  2616  610s\n",
      "H  257   239                       0.7418730    0.00007   100%  2485  610s\n",
      "   280   253     cutoff   59         0.74187    0.00010   100%  2368  634s\n",
      "   315   286 infeasible   10         0.74187    0.00010   100%  2249  641s\n",
      "   364   302    0.02092    9    7    0.74187    0.00010   100%  2029  688s\n",
      "   413   312    0.00016    7   26    0.74187    0.00010   100%  2005  710s\n",
      "H  436   310                       0.7417730    0.00010   100%  1958  710s\n",
      "   439   332    0.00025    8   21    0.74177    0.00010   100%  1970  723s\n",
      "   467   354    0.18003   10   16    0.74177    0.00010   100%  1934  749s\n",
      "H  494   305                       0.7080652    0.00010   100%  1908  749s\n",
      "   495   316    0.52844   30   12    0.70807    0.00010   100%  1904  777s\n",
      "   545   326    0.00016    6   30    0.70807    0.00010   100%  1840  796s\n",
      "   569   327    0.36981   16   11    0.70807    0.00010   100%  1837  835s\n",
      "   578   335    0.00010    6    6    0.70807    0.00010   100%  1952  861s\n",
      "   586   347    0.00010    8    6    0.70807    0.00010   100%  2040  886s\n",
      "   598   379    0.00020   13   19    0.70807    0.00010   100%  2095  900s\n",
      "\n",
      "Cutting planes:\n",
      "  Implied bound: 467\n",
      "  MIR: 269\n",
      "  Flow cover: 26\n",
      "  RLT: 16\n",
      "  Relax-and-lift: 414\n",
      "\n",
      "Explored 652 nodes (1364247 simplex iterations) in 900.05 seconds (2150.19 work units)\n",
      "Thread count was 8 (of 8 available processors)\n",
      "\n",
      "Solution count 6: 0.708065 0.741773 0.741873 ... 0.798103\n",
      "\n",
      "Time limit reached\n",
      "Best objective 7.080651685393e-01, best bound 1.000000000000e-04, gap 99.9859%\n",
      "WARNING: Loading a SolverResults object with an 'aborted' status, but\n",
      "containing a solution\n",
      "Solver running time: 901.3855607509613\n",
      "Solver termination condition: maxTimeLimit\n",
      "Valid Tree : Yes\n",
      "Set parameter WLSAccessID\n",
      "Set parameter WLSSecret\n",
      "Set parameter LicenseID to value 2433923\n",
      "WLS license - registered to pranav.chaudhary@sagataltd.io\n",
      "Read LP format model from file /var/folders/kb/sxk47lqj0xd8j0x92mykb0rr0000gn/T/tmpc4ommc7c.pyomo.lp\n",
      "Reading time = 0.27 seconds\n",
      "x1: 26210 rows, 6622 columns, 438053 nonzeros\n",
      "Set parameter TimeLimit to value 900\n",
      "Set parameter MIPGap to value 0.05\n",
      "Gurobi Optimizer version 10.0.3 build v10.0.3rc0 (mac64[x86])\n",
      "\n",
      "CPU model: Intel(R) Core(TM) i5-1038NG7 CPU @ 2.00GHz\n",
      "Thread count: 4 physical cores, 8 logical processors, using up to 8 threads\n",
      "\n",
      "WLS license - registered to pranav.chaudhary@sagataltd.io\n",
      "Optimize a model with 26210 rows, 6622 columns and 438053 nonzeros\n",
      "Model fingerprint: 0xc2fc6eb4\n",
      "Variable types: 6489 continuous, 133 integer (133 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+03]\n",
      "  Objective range  [3e-05, 1e-02]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 1e+03]\n",
      "Presolve removed 2026 rows and 451 columns\n",
      "Presolve time: 1.91s\n",
      "Presolved: 24184 rows, 6171 columns, 422661 nonzeros\n",
      "Variable types: 6044 continuous, 127 integer (127 binary)\n",
      "Found heuristic solution: objective 0.7979628\n",
      "\n",
      "Root simplex log...\n",
      "\n",
      "Iteration    Objective       Primal Inf.    Dual Inf.      Time\n",
      "    3033    8.7522529e-04   6.547769e+04   0.000000e+00      5s\n",
      "    7498    3.0000000e-05   0.000000e+00   0.000000e+00     10s\n",
      "\n",
      "Root relaxation: objective 3.000000e-05, 7498 iterations, 7.62 seconds (14.90 work units)\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "     0     0    0.00003    0    5    0.79796    0.00003   100%     -    9s\n",
      "     0     0    0.00003    0    4    0.79796    0.00003   100%     -   38s\n",
      "     0     0    0.00003    0    4    0.79796    0.00003   100%     -   42s\n",
      "     0     0    0.00003    0    6    0.79796    0.00003   100%     -   64s\n",
      "H    0     0                       0.7754909    0.00003   100%     -   65s\n",
      "     0     0    0.00003    0    5    0.77549    0.00003   100%     -   71s\n",
      "     0     0    0.00003    0    5    0.77549    0.00003   100%     -  115s\n",
      "     0     0    0.00003    0    5    0.77549    0.00003   100%     -  120s\n",
      "     0     0    0.00003    0    5    0.77549    0.00003   100%     -  148s\n",
      "     0     0    0.00003    0    5    0.77549    0.00003   100%     -  156s\n",
      "     0     0    0.00003    0    7    0.77549    0.00003   100%     -  182s\n",
      "     0     0    0.00003    0    6    0.77549    0.00003   100%     -  187s\n",
      "     0     0    0.00003    0   30    0.77549    0.00003   100%     -  262s\n",
      "     0     0    0.00003    0   29    0.77549    0.00003   100%     -  264s\n",
      "H    0     2                       0.7530190    0.00003   100%     -  288s\n",
      "     0     2    0.00003    0   29    0.75302    0.00003   100%     -  288s\n",
      "     1     4    0.00005    1   20    0.75302    0.00003   100% 31637  340s\n",
      "     3     8    0.00005    2   18    0.75302    0.00005   100% 19691  378s\n",
      "     7    12    0.00173    3    9    0.75302    0.00006   100% 11909  404s\n",
      "    11    16    0.00008    3   15    0.75302    0.00008   100% 11197  448s\n",
      "    15    20    0.00008    4   15    0.75302    0.00008   100% 11952  454s\n",
      "    19    24    0.00009    5   15    0.75302    0.00008   100%  9869  514s\n",
      "    23    30    0.28099    6   12    0.75302    0.00008   100%  9285  519s\n",
      "H   29    36                       0.7305471    0.00008   100%  8010  527s\n",
      "    35    42    0.00009    8   12    0.73055    0.00008   100%  7218  549s\n",
      "    41    50    0.00009   10   14    0.73055    0.00008   100%  6884  555s\n",
      "    49    59    0.00009   12   11    0.73055    0.00008   100%  6120  560s\n",
      "    60    72    0.00009   14   11    0.73055    0.00008   100%  5199  567s\n",
      "    73    84    0.00009   16   11    0.73055    0.00008   100%  4465  575s\n",
      "    85    97    0.00072   19   12    0.73055    0.00008   100%  3977  582s\n",
      "    98   102    0.06751   21    7    0.73055    0.00008   100%  3585  592s\n",
      "   108   116    0.14616   24    8    0.73055    0.00008   100%  3357  619s\n",
      "   125   133    0.22481   28    9    0.73055    0.00008   100%  3185  623s\n",
      "H  145   136                       0.7080452    0.00008   100%  2813  650s\n",
      "   163   151    0.21366   21   24    0.70805    0.00008   100%  2725  661s\n",
      "   184   163 infeasible   26         0.70805    0.00008   100%  2600  675s\n",
      "   200   169    0.61816   26   19    0.70805    0.00008   100%  2457  681s\n",
      "   222   184    0.00009    6   19    0.70805    0.00008   100%  2317  695s\n",
      "   249   212     cutoff   14         0.70805    0.00008   100%  2175  701s\n",
      "   289   245    0.68548   11    7    0.70805    0.00009   100%  1961  705s\n",
      "   342   280 infeasible   13         0.70805    0.00009   100%  1700  733s\n",
      "   417   296    0.00011   14   13    0.70805    0.00009   100%  1553  755s\n",
      "   433   417    0.00015   16   18    0.70805    0.00009   100%  1664  779s\n",
      "H  461   417                       0.7080152    0.00009   100%  1628  779s\n",
      "   586   471    0.24737   19   25    0.70802    0.00009   100%  1346  829s\n",
      "   725   484    0.00010    4   25    0.70802    0.00009   100%  1206  868s\n",
      "   738   565    0.00010    5   27    0.70802    0.00009   100%  1262  900s\n",
      "\n",
      "Cutting planes:\n",
      "  Implied bound: 212\n",
      "  MIR: 136\n",
      "  Flow cover: 7\n",
      "  RLT: 3\n",
      "  Relax-and-lift: 73\n",
      "\n",
      "Explored 883 nodes (1084589 simplex iterations) in 900.08 seconds (1504.56 work units)\n",
      "Thread count was 8 (of 8 available processors)\n",
      "\n",
      "Solution count 6: 0.708015 0.708045 0.730547 ... 0.797963\n",
      "\n",
      "Time limit reached\n",
      "Best objective 7.080151685393e-01, best bound 9.000000000000e-05, gap 99.9873%\n",
      "WARNING: Loading a SolverResults object with an 'aborted' status, but\n",
      "containing a solution\n",
      "Solver running time: 900.6868498325348\n",
      "Solver termination condition: maxTimeLimit\n",
      "Valid Tree : Yes\n",
      "Set parameter WLSAccessID\n",
      "Set parameter WLSSecret\n",
      "Set parameter LicenseID to value 2433923\n",
      "WLS license - registered to pranav.chaudhary@sagataltd.io\n",
      "Read LP format model from file /var/folders/kb/sxk47lqj0xd8j0x92mykb0rr0000gn/T/tmp351wep94.pyomo.lp\n",
      "Reading time = 0.45 seconds\n",
      "x1: 26210 rows, 6622 columns, 438053 nonzeros\n",
      "Set parameter TimeLimit to value 900\n",
      "Set parameter MIPGap to value 0.04\n",
      "Gurobi Optimizer version 10.0.3 build v10.0.3rc0 (mac64[x86])\n",
      "\n",
      "CPU model: Intel(R) Core(TM) i5-1038NG7 CPU @ 2.00GHz\n",
      "Thread count: 4 physical cores, 8 logical processors, using up to 8 threads\n",
      "\n",
      "WLS license - registered to pranav.chaudhary@sagataltd.io\n",
      "Optimize a model with 26210 rows, 6622 columns and 438053 nonzeros\n",
      "Model fingerprint: 0x8fe60270\n",
      "Variable types: 6489 continuous, 133 integer (133 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+03]\n",
      "  Objective range  [1e-04, 1e-02]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 1e+03]\n",
      "Presolve removed 2026 rows and 451 columns\n",
      "Presolve time: 2.85s\n",
      "Presolved: 24184 rows, 6171 columns, 422661 nonzeros\n",
      "Variable types: 6044 continuous, 127 integer (127 binary)\n",
      "Found heuristic solution: objective 0.7984528\n",
      "\n",
      "Root simplex log...\n",
      "\n",
      "Iteration    Objective       Primal Inf.    Dual Inf.      Time\n",
      "    1394    7.2422969e-04   4.844287e+05   0.000000e+00      5s\n",
      "    5051    1.0000000e-04   0.000000e+00   0.000000e+00     10s\n",
      "\n",
      "Root relaxation: objective 1.000000e-04, 5051 iterations, 6.51 seconds (9.30 work units)\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "     0     0    0.00010    0    5    0.79845    0.00010   100%     -    9s\n",
      "H    0     0                       0.7872169    0.00010   100%     -   10s\n",
      "     0     0    0.00010    0    7    0.78722    0.00010   100%     -   55s\n",
      "     0     0    0.00010    0    7    0.78722    0.00010   100%     -   57s\n",
      "     0     0    0.00010    0    5    0.78722    0.00010   100%     -   99s\n",
      "     0     0    0.00010    0    5    0.78722    0.00010   100%     -  108s\n",
      "     0     0    0.00010    0    5    0.78722    0.00010   100%     -  139s\n",
      "     0     0    0.00010    0    5    0.78722    0.00010   100%     -  148s\n",
      "     0     0    0.00010    0    4    0.78722    0.00010   100%     -  171s\n",
      "H    0     0                       0.7647449    0.00010   100%     -  174s\n",
      "     0     0    0.00010    0    6    0.76474    0.00010   100%     -  187s\n",
      "     0     0    0.00010    0    7    0.76474    0.00010   100%     -  212s\n",
      "     0     0    0.00010    0    7    0.76474    0.00010   100%     -  233s\n",
      "     0     0    0.00010    0    6    0.76474    0.00010   100%     -  259s\n",
      "     0     0    0.00010    0    6    0.76474    0.00010   100%     -  260s\n",
      "H    0     0                       0.7646449    0.00010   100%     -  267s\n",
      "     0     2    0.00010    0    6    0.76464    0.00010   100%     -  315s\n",
      "     1     4    0.00010    1    5    0.76464    0.00010   100% 15208  357s\n",
      "     3     8    0.00015    2   30    0.76464    0.00010   100% 24905  423s\n",
      "     7    12    0.02980    3   35    0.76464    0.00015   100% 15988  440s\n",
      "    11    16    0.00020    3    8    0.76464    0.00016   100% 13083  461s\n",
      "    15    20    0.00029    4   16    0.76464    0.00029   100% 13785  550s\n",
      "    19    24    0.00029    5    7    0.76464    0.00029   100% 11354  560s\n",
      "    23    30    0.00029    6   12    0.76464    0.00029   100% 10291  578s\n",
      "H   29    35                       0.7421730    0.00029   100%  9181  604s\n",
      "    35    41    0.00029    8   11    0.74217    0.00029   100%  9207  633s\n",
      "    41    49    0.00029    9    6    0.74217    0.00029   100%  8416  661s\n",
      "    49    55    0.00070   11   27    0.74217    0.00029   100%  7982  703s\n",
      "    55    63    0.00030   11    8    0.74217    0.00029   100%  7746  711s\n",
      "H   64    79                       0.7084652    0.00029   100%  6932  748s\n",
      "    87    85    0.00030   13   10    0.70847    0.00029   100%  5774  752s\n",
      "    99    92    0.00040   17   15    0.70847    0.00029   100%  5335  763s\n",
      "   108   110    0.00040   17   12    0.70847    0.00029   100%  5203  782s\n",
      "   126   133    0.00040   18   11    0.70847    0.00029   100%  4946  804s\n",
      "H  153   177                       0.7083652    0.00029   100%  4513  814s\n",
      "   203   222    0.29253   29    7    0.70837    0.00029   100%  3706  834s\n",
      "   265   250    0.00029    4   11    0.70837    0.00029   100%  3082  846s\n",
      "   295   276    0.00029    5   10    0.70837    0.00029   100%  2857  873s\n",
      "H  296   276                       0.7082652    0.00029   100%  2847  873s\n",
      "   325   309    0.00029    6    7    0.70827    0.00029   100%  2622  879s\n",
      "   368   335    0.00029    8    8    0.70827    0.00029   100%  2392  893s\n",
      "   425   336    0.00029    7   13    0.70827    0.00029   100%  2180  900s\n",
      "\n",
      "Cutting planes:\n",
      "  Implied bound: 561\n",
      "  MIR: 250\n",
      "  Flow cover: 31\n",
      "  RLT: 7\n",
      "  Relax-and-lift: 380\n",
      "\n",
      "Explored 438 nodes (1028579 simplex iterations) in 900.04 seconds (1969.54 work units)\n",
      "Thread count was 8 (of 8 available processors)\n",
      "\n",
      "Solution count 8: 0.708265 0.708365 0.708465 ... 0.798453\n",
      "\n",
      "Time limit reached\n",
      "Best objective 7.082651685393e-01, best bound 2.913105767600e-04, gap 99.9589%\n",
      "WARNING: Loading a SolverResults object with an 'aborted' status, but\n",
      "containing a solution\n",
      "Solver running time: 900.7432799339294\n",
      "Solver termination condition: maxTimeLimit\n",
      "Valid Tree : Yes\n",
      "Set parameter WLSAccessID\n",
      "Set parameter WLSSecret\n",
      "Set parameter LicenseID to value 2433923\n",
      "WLS license - registered to pranav.chaudhary@sagataltd.io\n",
      "Read LP format model from file /var/folders/kb/sxk47lqj0xd8j0x92mykb0rr0000gn/T/tmpnofgsw2w.pyomo.lp\n",
      "Reading time = 0.28 seconds\n",
      "x1: 26210 rows, 6622 columns, 438053 nonzeros\n",
      "Set parameter TimeLimit to value 900\n",
      "Set parameter MIPGap to value 0.1\n",
      "Gurobi Optimizer version 10.0.3 build v10.0.3rc0 (mac64[x86])\n",
      "\n",
      "CPU model: Intel(R) Core(TM) i5-1038NG7 CPU @ 2.00GHz\n",
      "Thread count: 4 physical cores, 8 logical processors, using up to 8 threads\n",
      "\n",
      "WLS license - registered to pranav.chaudhary@sagataltd.io\n",
      "Optimize a model with 26210 rows, 6622 columns and 438053 nonzeros\n",
      "Model fingerprint: 0xbedeb36a\n",
      "Variable types: 6489 continuous, 133 integer (133 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+03]\n",
      "  Objective range  [8e-05, 1e-02]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 1e+03]\n",
      "Presolve removed 2026 rows and 451 columns\n",
      "Presolve time: 1.91s\n",
      "Presolved: 24184 rows, 6171 columns, 422661 nonzeros\n",
      "Variable types: 6044 continuous, 127 integer (127 binary)\n",
      "Found heuristic solution: objective 0.7983128\n",
      "\n",
      "Root simplex log...\n",
      "\n",
      "Iteration    Objective       Primal Inf.    Dual Inf.      Time\n",
      "    2307    9.0815773e-04   3.444468e+04   0.000000e+00      5s\n",
      "    4296    8.0000000e-05   0.000000e+00   0.000000e+00      7s\n",
      "\n",
      "Root relaxation: objective 8.000000e-05, 4296 iterations, 3.99 seconds (7.43 work units)\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "     0     0    0.00008    0    5    0.79831    0.00008   100%     -    7s\n",
      "     0     0    0.00008    0    5    0.79831    0.00008   100%     -   46s\n",
      "     0     0    0.00008    0    5    0.79831    0.00008   100%     -   48s\n",
      "     0     0    0.00008    0    5    0.79831    0.00008   100%     -   70s\n",
      "H    0     0                       0.7758409    0.00008   100%     -   72s\n",
      "     0     0    0.00008    0    5    0.77584    0.00008   100%     -   77s\n",
      "     0     0    0.00008    0    4    0.77584    0.00008   100%     -  109s\n",
      "     0     0    0.00008    0    4    0.77584    0.00008   100%     -  127s\n",
      "     0     0    0.00008    0    6    0.77584    0.00008   100%     -  175s\n",
      "H    0     0                       0.7757609    0.00008   100%     -  176s\n",
      "     0     0    0.00008    0    6    0.77576    0.00008   100%     -  187s\n",
      "     0     0    0.00008    0    5    0.77576    0.00008   100%     -  217s\n",
      "     0     0    0.00008    0    5    0.77576    0.00008   100%     -  244s\n",
      "     0     0    0.00008    0    8    0.77576    0.00008   100%     -  289s\n",
      "     0     0    0.00008    0    8    0.77576    0.00008   100%     -  290s\n",
      "     0     2    0.00008    0    8    0.77576    0.00008   100%     -  348s\n",
      "     1     4    0.00008    1    9    0.77576    0.00008   100% 13490  402s\n",
      "     3     8    0.00016    2    8    0.77576    0.00008   100% 19867  435s\n",
      "     7    12    0.00016    3   14    0.77576    0.00009   100% 12923  447s\n",
      "    11    15    0.00016    4   14    0.77576    0.00009   100%  9391  457s\n",
      "    15    19    0.00024    4   15    0.77576    0.00016   100%  8519  489s\n",
      "    19    22    0.06650    5   14    0.77576    0.00016   100%  7699  496s\n",
      "    23    28    0.00018    5   10    0.77576    0.00016   100%  7340  503s\n",
      "H   29    33                       0.7308171    0.00016   100%  6302  511s\n",
      "    34    39    0.00024    7    8    0.73082    0.00016   100%  5934  522s\n",
      "    40    45    0.00024    8   12    0.73082    0.00016   100%  5750  528s\n",
      "    46    51    0.00024    9   12    0.73082    0.00016   100%  5427  535s\n",
      "    59    66    0.00024   10   14    0.73082    0.00016   100%  4848  543s\n",
      "H   63    66                       0.7083452    0.00016   100%  4551  543s\n",
      "    67    77    0.00032   10   18    0.70835    0.00016   100%  4583  550s\n",
      "    78    80    0.00032   11   17    0.70835    0.00016   100%  4158  561s\n",
      "    88    90    0.00032   12   17    0.70835    0.00016   100%  3940  567s\n",
      "   100    98    0.15773   13   22    0.70835    0.00016   100%  3797  579s\n",
      "   111   108    0.00032   14   14    0.70835    0.00016   100%  3608  587s\n",
      "   125   122    0.00032   15   13    0.70835    0.00016   100%  3407  594s\n",
      "   145   146     cutoff   17         0.70835    0.00016   100%  3138  599s\n",
      "   173   171    0.35987   23    8    0.70835    0.00016   100%  2732  604s\n",
      "H  192   171                       0.7082652    0.00016   100%  2495  604s\n",
      "   201   197    0.56212   28    8    0.70827    0.00016   100%  2408  613s\n",
      "H  241   216                       0.7081852    0.00016   100%  2084  631s\n",
      "   269   229    0.00016    5   10    0.70819    0.00016   100%  2006  655s\n",
      "   306   257    0.00016    6   10    0.70819    0.00016   100%  1819  663s\n",
      "   342   287    0.00016    7   13    0.70819    0.00016   100%  1695  667s\n",
      "   390   287    0.00024    8   12    0.70819    0.00016   100%  1556  681s\n",
      "   406   331    0.00024    9   10    0.70819    0.00016   100%  1563  690s\n",
      "H  433   331                       0.7081052    0.00016   100%  1531  690s\n",
      "   454   346    0.00024   11   10    0.70811    0.00016   100%  1487  711s\n",
      "   480   399    0.00024   12    9    0.70811    0.00016   100%  1416  716s\n",
      "   556   402    0.00040   31   10    0.70811    0.00016   100%  1269 1239s\n",
      "\n",
      "Cutting planes:\n",
      "  Implied bound: 270\n",
      "  MIR: 126\n",
      "  Flow cover: 7\n",
      "  RLT: 5\n",
      "  Relax-and-lift: 212\n",
      "\n",
      "Explored 559 nodes (814119 simplex iterations) in 1239.15 seconds (1361.82 work units)\n",
      "Thread count was 8 (of 8 available processors)\n",
      "\n",
      "Solution count 8: 0.708105 0.708185 0.708265 ... 0.798313\n",
      "\n",
      "Time limit reached\n",
      "Best objective 7.081051685393e-01, best bound 1.600000000000e-04, gap 99.9774%\n",
      "WARNING: Loading a SolverResults object with an 'aborted' status, but\n",
      "containing a solution\n",
      "Solver running time: 1240.5591459274292\n",
      "Solver termination condition: maxTimeLimit\n",
      "Valid Tree : Yes\n",
      "pop_after_fit [{'min_samples_leaf': 9, 'mip_gap_tol': 0.1, 'alpha': 8e-05}, {'min_samples_leaf': 5, 'mip_gap_tol': 0.04, 'alpha': 0.0001}, {'min_samples_leaf': 7, 'mip_gap_tol': 0.05, 'alpha': 3e-05}, {'min_samples_leaf': 10, 'mip_gap_tol': 0.01, 'alpha': 5e-05}]\n",
      "Best score in generation 1 : [0.9219330855018587]\n",
      "Set parameter WLSAccessID\n",
      "Set parameter WLSSecret\n",
      "Set parameter LicenseID to value 2433923\n",
      "WLS license - registered to pranav.chaudhary@sagataltd.io\n",
      "Read LP format model from file /var/folders/kb/sxk47lqj0xd8j0x92mykb0rr0000gn/T/tmp2fm565is.pyomo.lp\n",
      "Reading time = 0.18 seconds\n",
      "x1: 26210 rows, 6622 columns, 438053 nonzeros\n",
      "Set parameter TimeLimit to value 900\n",
      "Set parameter MIPGap to value 0.1\n",
      "Gurobi Optimizer version 10.0.3 build v10.0.3rc0 (mac64[x86])\n",
      "\n",
      "CPU model: Intel(R) Core(TM) i5-1038NG7 CPU @ 2.00GHz\n",
      "Thread count: 4 physical cores, 8 logical processors, using up to 8 threads\n",
      "\n",
      "WLS license - registered to pranav.chaudhary@sagataltd.io\n",
      "Optimize a model with 26210 rows, 6622 columns and 438053 nonzeros\n",
      "Model fingerprint: 0xbedeb36a\n",
      "Variable types: 6489 continuous, 133 integer (133 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+03]\n",
      "  Objective range  [8e-05, 1e-02]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 1e+03]\n",
      "Presolve removed 2026 rows and 451 columns\n",
      "Presolve time: 1.57s\n",
      "Presolved: 24184 rows, 6171 columns, 422661 nonzeros\n",
      "Variable types: 6044 continuous, 127 integer (127 binary)\n",
      "Found heuristic solution: objective 0.7983128\n",
      "\n",
      "Root simplex log...\n",
      "\n",
      "Iteration    Objective       Primal Inf.    Dual Inf.      Time\n",
      "    3897    1.0367795e-03   1.303765e+03   0.000000e+00      5s\n",
      "    4296    8.0000000e-05   0.000000e+00   0.000000e+00      6s\n",
      "\n",
      "Root relaxation: objective 8.000000e-05, 4296 iterations, 3.66 seconds (7.43 work units)\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "     0     0    0.00008    0    5    0.79831    0.00008   100%     -    5s\n",
      "     0     0    0.00008    0    5    0.79831    0.00008   100%     -   38s\n",
      "     0     0    0.00008    0    5    0.79831    0.00008   100%     -   41s\n",
      "     0     0    0.00008    0    5    0.79831    0.00008   100%     -   62s\n",
      "H    0     0                       0.7758409    0.00008   100%     -   64s\n",
      "     0     0    0.00008    0    5    0.77584    0.00008   100%     -   68s\n",
      "     0     0    0.00008    0    4    0.77584    0.00008   100%     -   90s\n",
      "     0     0    0.00008    0    4    0.77584    0.00008   100%     -  104s\n",
      "     0     0    0.00008    0    6    0.77584    0.00008   100%     -  140s\n",
      "H    0     0                       0.7757609    0.00008   100%     -  141s\n",
      "     0     0    0.00008    0    6    0.77576    0.00008   100%     -  149s\n",
      "     0     0    0.00008    0    5    0.77576    0.00008   100%     -  169s\n",
      "     0     0    0.00008    0    5    0.77576    0.00008   100%     -  190s\n",
      "     0     0    0.00008    0    8    0.77576    0.00008   100%     -  228s\n",
      "     0     0    0.00008    0    8    0.77576    0.00008   100%     -  229s\n",
      "     0     2    0.00008    0    8    0.77576    0.00008   100%     -  286s\n",
      "     1     4    0.00008    1    9    0.77576    0.00008   100% 13490  335s\n",
      "     3     8    0.00016    2    8    0.77576    0.00008   100% 19867  366s\n",
      "     7    12    0.00016    3   14    0.77576    0.00009   100% 12923  377s\n",
      "    11    15    0.00016    4   14    0.77576    0.00009   100%  9391  387s\n",
      "    15    19    0.00024    4   15    0.77576    0.00016   100%  8519  417s\n",
      "    19    22    0.06650    5   14    0.77576    0.00016   100%  7699  424s\n",
      "    23    28    0.00018    5   10    0.77576    0.00016   100%  7340  431s\n",
      "H   29    33                       0.7308171    0.00016   100%  6302  439s\n",
      "    34    39    0.00024    7    8    0.73082    0.00016   100%  5934  449s\n",
      "    40    45    0.00024    8   12    0.73082    0.00016   100%  5750  456s\n",
      "    46    51    0.00024    9   12    0.73082    0.00016   100%  5427  462s\n",
      "    52    58    0.00024    9   15    0.73082    0.00016   100%  5159  466s\n",
      "    59    66    0.00024   10   14    0.73082    0.00016   100%  4848  471s\n",
      "H   63    66                       0.7083452    0.00016   100%  4551  471s\n",
      "    67    77    0.00032   10   18    0.70835    0.00016   100%  4583  477s\n",
      "    78    80    0.00032   11   17    0.70835    0.00016   100%  4158  487s\n",
      "    88    90    0.00032   12   17    0.70835    0.00016   100%  3940  493s\n",
      "   100    98    0.15773   13   22    0.70835    0.00016   100%  3797  505s\n",
      "   111   108    0.00032   14   14    0.70835    0.00016   100%  3608  512s\n",
      "   125   122    0.00032   15   13    0.70835    0.00016   100%  3407  519s\n",
      "   145   146     cutoff   17         0.70835    0.00016   100%  3138  524s\n",
      "   173   171    0.35987   23    8    0.70835    0.00016   100%  2732  530s\n",
      "H  192   171                       0.7082652    0.00016   100%  2495  530s\n",
      "   201   197    0.56212   28    8    0.70827    0.00016   100%  2408  538s\n",
      "H  241   216                       0.7081852    0.00016   100%  2084  556s\n",
      "   269   229    0.00016    5   10    0.70819    0.00016   100%  2006  579s\n",
      "   306   257    0.00016    6   10    0.70819    0.00016   100%  1819  587s\n",
      "   342   287    0.00016    7   13    0.70819    0.00016   100%  1695  592s\n",
      "   390   287    0.00024    8   12    0.70819    0.00016   100%  1556  606s\n",
      "   406   331    0.00024    9   10    0.70819    0.00016   100%  1563  615s\n",
      "H  433   331                       0.7081052    0.00016   100%  1531  615s\n",
      "   454   346    0.00024   11   10    0.70811    0.00016   100%  1487  634s\n",
      "   480   399    0.00024   12    9    0.70811    0.00016   100%  1416  639s\n",
      "   556   402    0.00024   17   11    0.70811    0.00016   100%  1275  675s\n",
      "   569   419    0.00024   18    7    0.70811    0.00016   100%  1299  704s\n",
      "   601   444    0.00032   20    9    0.70811    0.00016   100%  1350  753s\n",
      "   650   457    0.50915   44    6    0.70811    0.00016   100%  1327  777s\n",
      "   663   458    0.65201   48    7    0.70811    0.00016   100%  1351  900s\n",
      "\n",
      "Cutting planes:\n",
      "  Implied bound: 293\n",
      "  MIR: 142\n",
      "  Flow cover: 7\n",
      "  RLT: 5\n",
      "  Relax-and-lift: 230\n",
      "\n",
      "Explored 684 nodes (1020793 simplex iterations) in 900.39 seconds (1642.79 work units)\n",
      "Thread count was 8 (of 8 available processors)\n",
      "\n",
      "Solution count 8: 0.708105 0.708185 0.708265 ... 0.798313\n",
      "\n",
      "Time limit reached\n",
      "Best objective 7.081051685393e-01, best bound 1.600000000000e-04, gap 99.9774%\n",
      "WARNING: Loading a SolverResults object with an 'aborted' status, but\n",
      "containing a solution\n",
      "Solver running time: 903.4445610046387\n",
      "Solver termination condition: maxTimeLimit\n",
      "Valid Tree : Yes\n",
      "Set parameter WLSAccessID\n",
      "Set parameter WLSSecret\n",
      "Set parameter LicenseID to value 2433923\n",
      "WLS license - registered to pranav.chaudhary@sagataltd.io\n",
      "Read LP format model from file /var/folders/kb/sxk47lqj0xd8j0x92mykb0rr0000gn/T/tmpri47ch8y.pyomo.lp\n",
      "Reading time = 0.24 seconds\n",
      "x1: 26210 rows, 6622 columns, 438053 nonzeros\n",
      "Set parameter TimeLimit to value 900\n",
      "Set parameter MIPGap to value 0.04\n",
      "Gurobi Optimizer version 10.0.3 build v10.0.3rc0 (mac64[x86])\n",
      "\n",
      "CPU model: Intel(R) Core(TM) i5-1038NG7 CPU @ 2.00GHz\n",
      "Thread count: 4 physical cores, 8 logical processors, using up to 8 threads\n",
      "\n",
      "WLS license - registered to pranav.chaudhary@sagataltd.io\n",
      "Optimize a model with 26210 rows, 6622 columns and 438053 nonzeros\n",
      "Model fingerprint: 0x8fe60270\n",
      "Variable types: 6489 continuous, 133 integer (133 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+03]\n",
      "  Objective range  [1e-04, 1e-02]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 1e+03]\n",
      "Presolve removed 2026 rows and 451 columns\n",
      "Presolve time: 1.90s\n",
      "Presolved: 24184 rows, 6171 columns, 422661 nonzeros\n",
      "Variable types: 6044 continuous, 127 integer (127 binary)\n",
      "Found heuristic solution: objective 0.7984528\n",
      "\n",
      "Root simplex log...\n",
      "\n",
      "Iteration    Objective       Primal Inf.    Dual Inf.      Time\n",
      "    2584    9.4416354e-04   5.524370e+04   0.000000e+00      5s\n",
      "    5051    1.0000000e-04   0.000000e+00   0.000000e+00      8s\n",
      "\n",
      "Root relaxation: objective 1.000000e-04, 5051 iterations, 5.97 seconds (9.30 work units)\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "     0     0    0.00010    0    5    0.79845    0.00010   100%     -    8s\n",
      "H    0     0                       0.7872169    0.00010   100%     -    9s\n",
      "     0     0    0.00010    0    7    0.78722    0.00010   100%     -   48s\n",
      "     0     0    0.00010    0    7    0.78722    0.00010   100%     -   50s\n",
      "     0     0    0.00010    0    5    0.78722    0.00010   100%     -   88s\n",
      "     0     0    0.00010    0    5    0.78722    0.00010   100%     -   95s\n",
      "     0     0    0.00010    0    5    0.78722    0.00010   100%     -  125s\n",
      "     0     0    0.00010    0    5    0.78722    0.00010   100%     -  131s\n",
      "     0     0    0.00010    0    4    0.78722    0.00010   100%     -  148s\n",
      "H    0     0                       0.7647449    0.00010   100%     -  150s\n",
      "     0     0    0.00010    0    6    0.76474    0.00010   100%     -  160s\n",
      "     0     0    0.00010    0    7    0.76474    0.00010   100%     -  180s\n",
      "     0     0    0.00010    0    7    0.76474    0.00010   100%     -  197s\n",
      "     0     0    0.00010    0    6    0.76474    0.00010   100%     -  217s\n",
      "     0     0    0.00010    0    6    0.76474    0.00010   100%     -  217s\n",
      "H    0     0                       0.7646449    0.00010   100%     -  221s\n",
      "     0     2    0.00010    0    6    0.76464    0.00010   100%     -  264s\n",
      "     1     4    0.00010    1    5    0.76464    0.00010   100% 15208  295s\n",
      "     3     8    0.00015    2   30    0.76464    0.00010   100% 24905  355s\n",
      "     7    12    0.02980    3   35    0.76464    0.00015   100% 15988  369s\n",
      "    11    16    0.00020    3    8    0.76464    0.00016   100% 13083  382s\n",
      "    15    20    0.00029    4   16    0.76464    0.00029   100% 13785  462s\n",
      "    19    24    0.00029    5    7    0.76464    0.00029   100% 11354  471s\n",
      "    23    30    0.00029    6   12    0.76464    0.00029   100% 10291  485s\n",
      "H   29    35                       0.7421730    0.00029   100%  9181  511s\n",
      "    35    41    0.00029    8   11    0.74217    0.00029   100%  9207  538s\n",
      "    41    49    0.00029    9    6    0.74217    0.00029   100%  8416  567s\n",
      "    49    55    0.00070   11   27    0.74217    0.00029   100%  7982  603s\n",
      "    55    63    0.00030   11    8    0.74217    0.00029   100%  7746  612s\n",
      "H   64    79                       0.7084652    0.00029   100%  6932  635s\n",
      "    87    85    0.00030   13   10    0.70847    0.00029   100%  5774  640s\n",
      "    99    92    0.00040   17   15    0.70847    0.00029   100%  5335  649s\n",
      "   108   110    0.00040   17   12    0.70847    0.00029   100%  5203  664s\n",
      "   126   133    0.00040   18   11    0.70847    0.00029   100%  4946  680s\n",
      "H  153   177                       0.7083652    0.00029   100%  4513  690s\n",
      "   203   222    0.29253   29    7    0.70837    0.00029   100%  3706  709s\n",
      "   265   250    0.00029    4   11    0.70837    0.00029   100%  3082  719s\n",
      "   295   276    0.00029    5   10    0.70837    0.00029   100%  2857  741s\n",
      "H  296   276                       0.7082652    0.00029   100%  2847  741s\n",
      "   325   309    0.00029    6    7    0.70827    0.00029   100%  2622  747s\n",
      "   368   335    0.00029    8    8    0.70827    0.00029   100%  2392  758s\n",
      "   425   347    0.00030   10   19    0.70827    0.00029   100%  2189  769s\n",
      "   461   407    0.00030   11   20    0.70827    0.00029   100%  2083  781s\n",
      "   540   441    0.00030   13   10    0.70827    0.00029   100%  1862  799s\n",
      "   589   445    0.00030   14   10    0.70827    0.00029   100%  1791  815s\n",
      "   606   480    0.20255   18    7    0.70827    0.00029   100%  1805  838s\n",
      "   653   504    0.20255   27    6    0.70827    0.00029   100%  1759  872s\n",
      "   716   546    0.00030    7    9    0.70827    0.00029   100%  1729  900s\n",
      "\n",
      "Cutting planes:\n",
      "  Implied bound: 900\n",
      "  MIR: 290\n",
      "  Flow cover: 35\n",
      "  RLT: 11\n",
      "  Relax-and-lift: 382\n",
      "\n",
      "Explored 776 nodes (1419627 simplex iterations) in 900.04 seconds (2276.62 work units)\n",
      "Thread count was 8 (of 8 available processors)\n",
      "\n",
      "Solution count 8: 0.708265 0.708365 0.708465 ... 0.798453\n",
      "\n",
      "Time limit reached\n",
      "Best objective 7.082651685393e-01, best bound 2.913105767600e-04, gap 99.9589%\n",
      "WARNING: Loading a SolverResults object with an 'aborted' status, but\n",
      "containing a solution\n",
      "Solver running time: 901.4465198516846\n",
      "Solver termination condition: maxTimeLimit\n",
      "Valid Tree : Yes\n",
      "Set parameter WLSAccessID\n",
      "Set parameter WLSSecret\n",
      "Set parameter LicenseID to value 2433923\n",
      "WLS license - registered to pranav.chaudhary@sagataltd.io\n",
      "Read LP format model from file /var/folders/kb/sxk47lqj0xd8j0x92mykb0rr0000gn/T/tmpmu4vmeig.pyomo.lp\n",
      "Reading time = 0.20 seconds\n",
      "x1: 26210 rows, 6622 columns, 438053 nonzeros\n",
      "Set parameter TimeLimit to value 900\n",
      "Set parameter MIPGap to value 0.1\n",
      "Gurobi Optimizer version 10.0.3 build v10.0.3rc0 (mac64[x86])\n",
      "\n",
      "CPU model: Intel(R) Core(TM) i5-1038NG7 CPU @ 2.00GHz\n",
      "Thread count: 4 physical cores, 8 logical processors, using up to 8 threads\n",
      "\n",
      "WLS license - registered to pranav.chaudhary@sagataltd.io\n",
      "Optimize a model with 26210 rows, 6622 columns and 438053 nonzeros\n",
      "Model fingerprint: 0x9990f4e5\n",
      "Variable types: 6489 continuous, 133 integer (133 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+03]\n",
      "  Objective range  [1e-04, 1e-02]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 1e+03]\n",
      "Presolve removed 2026 rows and 451 columns\n",
      "Presolve time: 1.83s\n",
      "Presolved: 24184 rows, 6171 columns, 422661 nonzeros\n",
      "Variable types: 6044 continuous, 127 integer (127 binary)\n",
      "Found heuristic solution: objective 0.7984528\n",
      "\n",
      "Root simplex log...\n",
      "\n",
      "Iteration    Objective       Primal Inf.    Dual Inf.      Time\n",
      "    2804    9.5491859e-04   2.553891e+04   0.000000e+00      5s\n",
      "    6277    1.0000000e-04   0.000000e+00   0.000000e+00      9s\n",
      "\n",
      "Root relaxation: objective 1.000000e-04, 6277 iterations, 7.33 seconds (13.23 work units)\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "     0     0    0.00010    0    5    0.79845    0.00010   100%     -    9s\n",
      "H    0     0                       0.7872169    0.00010   100%     -    9s\n",
      "     0     0    0.00010    0    6    0.78722    0.00010   100%     -   36s\n",
      "     0     0    0.00010    0    5    0.78722    0.00010   100%     -   39s\n",
      "     0     0    0.00010    0    5    0.78722    0.00010   100%     -   63s\n",
      "     0     0    0.00010    0    5    0.78722    0.00010   100%     -   83s\n",
      "     0     0    0.00010    0    6    0.78722    0.00010   100%     -  112s\n",
      "     0     0    0.00010    0    5    0.78722    0.00010   100%     -  119s\n",
      "     0     0    0.00010    0    8    0.78722    0.00010   100%     -  139s\n",
      "H    0     0                       0.7647449    0.00010   100%     -  140s\n",
      "     0     0    0.00010    0    6    0.76474    0.00010   100%     -  147s\n",
      "     0     0    0.00010    0    6    0.76474    0.00010   100%     -  167s\n",
      "     0     0    0.00010    0    6    0.76474    0.00010   100%     -  177s\n",
      "     0     0    0.00010    0    7    0.76474    0.00010   100%     -  202s\n",
      "     0     0    0.00010    0    7    0.76474    0.00010   100%     -  203s\n",
      "     0     2    0.00010    0    7    0.76474    0.00010   100%     -  251s\n",
      "     1     4    0.00010    1    4    0.76474    0.00010   100%  6012  296s\n",
      "     3     8    0.00015    2   43    0.76474    0.00010   100% 28192  437s\n",
      "     7    12    0.00355    3   45    0.76474    0.00012   100% 29389  451s\n",
      "    11    16    0.00020    3    9    0.76474    0.00012   100% 20478  467s\n",
      "    15    20    0.00070    4   37    0.76474    0.00012   100% 17464  543s\n",
      "    19    24    0.00070    5   40    0.76474    0.00012   100% 14568  552s\n",
      "    23    30    0.00070    6   29    0.76474    0.00012   100% 12871  562s\n",
      "    29    37    0.00070    7   36    0.76474    0.00012   100% 11585  567s\n",
      "H   33    37                       0.7535090    0.00012   100% 10558  567s\n",
      "    36    44    0.00070    8   36    0.75351    0.00012   100% 10074  571s\n",
      "    43    53    0.00070   10   29    0.75351    0.00012   100%  8696  576s\n",
      "    52    63    0.00070   13   19    0.75351    0.00012   100%  7542  580s\n",
      "    74    87    0.00070   18   23    0.75351    0.00012   100%  5828  590s\n",
      "   102   115    0.00070   26   21    0.75351    0.00012   100%  4590  595s\n",
      "H  114   115                       0.7534090    0.00012   100%  4155  595s\n",
      "   143   125    0.00060   35   13    0.75341    0.00012   100%  3499  607s\n",
      "   150   133    0.00060   36   14    0.75341    0.00012   100%  3496  614s\n",
      "   158   151    0.02307   37   16    0.75341    0.00012   100%  3420  620s\n",
      "   198   178 infeasible   46         0.75341    0.00026   100%  2943  627s\n",
      "   223   202    0.00083    5    9    0.75341    0.00026   100%  2667  630s\n",
      "   256   210 infeasible   11         0.75341    0.00026   100%  2373  637s\n",
      "   285   227    0.00026    6    6    0.75341    0.00026   100%  2223  647s\n",
      "   303   232    0.00030    7   19    0.75341    0.00026   100%  2170  654s\n",
      "   320   263    0.00040    9   13    0.75341    0.00026   100%  2142  658s\n",
      "H  353   284                       0.7533090    0.00026   100%  2015  671s\n",
      "   389   312    0.09294   13   15    0.75331    0.00026   100%  1945  677s\n",
      "   425   335    0.00026    8    8    0.75331    0.00026   100%  1875  686s\n",
      "   458   368    0.00026    9   19    0.75331    0.00026   100%  1795  691s\n",
      "H  493   387                       0.7532090    0.00026   100%  1732  702s\n",
      "   520   432    0.10634   17   20    0.75321    0.00026   100%  1684  709s\n",
      "H  587   433                       0.7307371    0.00026   100%  1578  732s\n",
      "   591   503    0.00040   10   15    0.73074    0.00026   100%  1587  736s\n",
      "H  679   487                       0.7082652    0.00026   100%  1422  736s\n",
      "   684   515 infeasible   35         0.70827    0.00026   100%  1412  743s\n",
      "   760   525    0.00030   10   22    0.70827    0.00026   100%  1307  752s\n",
      "   785   543    0.15849   18   15    0.70827    0.00026   100%  1329  774s\n",
      "   827   570    0.00040   11   14    0.70827    0.00026   100%  1329  784s\n",
      "   879   572    0.00042   14   21    0.70827    0.00026   100%  1306  794s\n",
      "   902   597    0.16059   13   17    0.70827    0.00026   100%  1323  802s\n",
      "H  960   597                       0.7081652    0.00026   100%  1300  802s\n",
      "   961   614    0.14541   28   23    0.70817    0.00027   100%  1298  812s\n",
      "  1009   618    0.11276   16   19    0.70817    0.00027   100%  1281  860s\n",
      "  1013   661    0.11276   17   18    0.70817    0.00027   100%  1288  900s\n",
      "\n",
      "Cutting planes:\n",
      "  Implied bound: 613\n",
      "  MIR: 202\n",
      "  Flow cover: 31\n",
      "  RLT: 10\n",
      "  Relax-and-lift: 174\n",
      "\n",
      "Explored 1100 nodes (1442760 simplex iterations) in 900.04 seconds (2164.85 work units)\n",
      "Thread count was 8 (of 8 available processors)\n",
      "\n",
      "Solution count 10: 0.708165 0.708265 0.730737 ... 0.798453\n",
      "\n",
      "Time limit reached\n",
      "Best objective 7.081651685393e-01, best bound 2.707983939176e-04, gap 99.9618%\n",
      "WARNING: Loading a SolverResults object with an 'aborted' status, but\n",
      "containing a solution\n",
      "Solver running time: 900.4599890708923\n",
      "Solver termination condition: maxTimeLimit\n",
      "Valid Tree : Yes\n",
      "pop_after_fit [{'min_samples_leaf': 9, 'mip_gap_tol': 0.1, 'alpha': 0.0001}, {'min_samples_leaf': 5, 'mip_gap_tol': 0.04, 'alpha': 0.0001}, {'min_samples_leaf': 9, 'mip_gap_tol': 0.1, 'alpha': 8e-05}]\n",
      "Best score in generation 2 : [0.9219330855018587]\n"
     ]
    }
   ],
   "source": [
    "# Run the GA\n",
    "# logmodel = BinNodePenaltyOptimalTreeClassifier(max_depth=max_depth,min_samples_leaf=min_samples_leaf,\n",
    "#                                   alpha=alpha,\n",
    "#                                   criterion=\"gini\",\n",
    "#                                   solver=\"gurobi\",\n",
    "#                                   time_limit=time_limit,\n",
    "#                                   verbose=True,\n",
    "#                                   warm_start=warm_start,\n",
    "#                                   log_file=log_file,\n",
    "#                                   solver_options={'mip_cuts': None,\n",
    "#                                                   'mip_gap_tol': mip_gap_tol,\n",
    "#                                                   'mip_focus': mip_focus,\n",
    "#                                                   'mip_polish_time': mip_polish_time,\n",
    "#                                                   }\n",
    "#                                   )\n",
    "# X_train,X_test, Y_train, Y_test = split(X, y)\n",
    "chromo_df_bc,score_bc=generations(select_X_train,y_train,size=4,n_feat=select_X_train.shape[1],n_parents=2,mutation_rate=0.00,n_gen=2,\n",
    "                         X_train = select_X_train,X_test = select_X_train,Y_train = y_train,Y_test = y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'min_samples_leaf': 9, 'mip_gap_tol': 0.1, 'alpha': 8e-05}, {'min_samples_leaf': 9, 'mip_gap_tol': 0.1, 'alpha': 0.0001}]\n",
      "[0.9219330855018587, 0.9219330855018587]\n"
     ]
    }
   ],
   "source": [
    "# Select the features indicated by the GA\n",
    "print(chromo_df_bc)\n",
    "print(score_bc)\n",
    "# names = []\n",
    "# select_X_train = []\n",
    "# for i in range(len(chromo_df_bc[2])):\n",
    "#     if chromo_df_bc[2][i] == True:\n",
    "#         names.append(fraud_data.iloc[:, [i]].columns[0])\n",
    "#         select_X_train.append(X_train[:, i])\n",
    "# select_X_train = np.asarray(select_X_train).T\n",
    "# print(len(names))\n",
    "# print(select_X_train.shape)\n",
    "# print(X_train.shape)\n",
    "# print(Y_train.shape)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(807, 5)\n",
      "(807, 7)\n"
     ]
    }
   ],
   "source": [
    "names_1 = []\n",
    "select_X_train_1 = []\n",
    "for i in range(len(chromo_df_bc[0])):\n",
    "    if chromo_df_bc[0][i] == True:\n",
    "        names_1.append(names[i])\n",
    "        select_X_train_1.append(select_X_train[:, i])\n",
    "select_X_train_1 = np.asarray(select_X_train_1).T\n",
    "print(select_X_train_1.shape)\n",
    "print(select_X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lsopt.tree import OptimalTreeClassifier ## M-OCT propsed by Liu & Allen\n",
    "from lsopt.tree import BinNodePenaltyOptimalTreeClassifier ## BNP-OCT propsed by Liu & Allen\n",
    "# selection = SelectFromModel(model, threshold=true_thresh, prefit=True)\n",
    "# select_X_train = selection.transform(X_train)\n",
    "\n",
    "# OCT parameters\n",
    "max_depth = 3\n",
    "min_samples_leaf = chromo_df_bc[0][\"min_samples_leaf\"]\n",
    "alpha = chromo_df_bc[0][\"alpha\"]\n",
    "time_limit = 25  # minute\n",
    "mip_gap_tol = chromo_df_bc[0][\"mip_gap_tol\"]  # optimal gap percentage\n",
    "mip_focus = 'balance'\n",
    "mip_polish_time = None\n",
    "fp_heur = True\n",
    "backtrack = \"bestb\"\n",
    "\n",
    "warm_start = False\n",
    "log_file = None\n",
    "\n",
    "\n",
    "\n",
    "# Construct BNP-OCT classifier\n",
    "selection_model = BinNodePenaltyOptimalTreeClassifier(max_depth=max_depth,min_samples_leaf=min_samples_leaf,\n",
    "                                  alpha=alpha,\n",
    "                                  criterion=\"gini\",\n",
    "                                  solver=\"gurobi\",\n",
    "                                  time_limit=time_limit,\n",
    "                                  verbose=True,\n",
    "                                  warm_start=warm_start,\n",
    "                                  log_file=log_file,\n",
    "                                  solver_options={'mip_cuts': None,\n",
    "                                                  'mip_gap_tol': mip_gap_tol,\n",
    "                                                  'mip_focus': mip_focus,\n",
    "                                                  'mip_polish_time': mip_polish_time,\n",
    "                                                  }\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set parameter WLSAccessID\n",
      "Set parameter WLSSecret\n",
      "Set parameter LicenseID to value 2433923\n",
      "WLS license - registered to pranav.chaudhary@sagataltd.io\n",
      "Read LP format model from file /var/folders/kb/sxk47lqj0xd8j0x92mykb0rr0000gn/T/tmpjnz7p66i.pyomo.lp\n",
      "Reading time = 0.18 seconds\n",
      "x1: 26210 rows, 6622 columns, 438053 nonzeros\n",
      "Set parameter TimeLimit to value 1500\n",
      "Set parameter MIPGap to value 0.1\n",
      "Gurobi Optimizer version 10.0.3 build v10.0.3rc0 (mac64[x86])\n",
      "\n",
      "CPU model: Intel(R) Core(TM) i5-1038NG7 CPU @ 2.00GHz\n",
      "Thread count: 4 physical cores, 8 logical processors, using up to 8 threads\n",
      "\n",
      "WLS license - registered to pranav.chaudhary@sagataltd.io\n",
      "Optimize a model with 26210 rows, 6622 columns and 438053 nonzeros\n",
      "Model fingerprint: 0xbedeb36a\n",
      "Variable types: 6489 continuous, 133 integer (133 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+03]\n",
      "  Objective range  [8e-05, 1e-02]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 1e+03]\n",
      "Presolve removed 2026 rows and 451 columns\n",
      "Presolve time: 1.52s\n",
      "Presolved: 24184 rows, 6171 columns, 422661 nonzeros\n",
      "Variable types: 6044 continuous, 127 integer (127 binary)\n",
      "Found heuristic solution: objective 0.7983128\n",
      "\n",
      "Root simplex log...\n",
      "\n",
      "Iteration    Objective       Primal Inf.    Dual Inf.      Time\n",
      "    3747    1.0353252e-03   6.688176e+03   0.000000e+00      5s\n",
      "    4296    8.0000000e-05   0.000000e+00   0.000000e+00      6s\n",
      "\n",
      "Root relaxation: objective 8.000000e-05, 4296 iterations, 3.85 seconds (7.43 work units)\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "     0     0    0.00008    0    5    0.79831    0.00008   100%     -    5s\n",
      "     0     0    0.00008    0    5    0.79831    0.00008   100%     -   37s\n",
      "     0     0    0.00008    0    5    0.79831    0.00008   100%     -   39s\n",
      "     0     0    0.00008    0    5    0.79831    0.00008   100%     -   59s\n",
      "H    0     0                       0.7758409    0.00008   100%     -   61s\n",
      "     0     0    0.00008    0    5    0.77584    0.00008   100%     -   64s\n",
      "     0     0    0.00008    0    4    0.77584    0.00008   100%     -   88s\n",
      "     0     0    0.00008    0    4    0.77584    0.00008   100%     -  104s\n",
      "     0     0    0.00008    0    6    0.77584    0.00008   100%     -  147s\n",
      "H    0     0                       0.7757609    0.00008   100%     -  148s\n",
      "     0     0    0.00008    0    6    0.77576    0.00008   100%     -  158s\n",
      "     0     0    0.00008    0    5    0.77576    0.00008   100%     -  182s\n",
      "     0     0    0.00008    0    5    0.77576    0.00008   100%     -  209s\n",
      "     0     0    0.00008    0    8    0.77576    0.00008   100%     -  253s\n",
      "     0     0    0.00008    0    8    0.77576    0.00008   100%     -  253s\n",
      "     0     2    0.00008    0    8    0.77576    0.00008   100%     -  313s\n",
      "     1     4    0.00008    1    9    0.77576    0.00008   100% 13490  372s\n",
      "     3     8    0.00016    2    8    0.77576    0.00008   100% 19867  403s\n",
      "     7    12    0.00016    3   14    0.77576    0.00009   100% 12923  415s\n",
      "    11    15    0.00016    4   14    0.77576    0.00009   100%  9391  428s\n",
      "    15    19    0.00024    4   15    0.77576    0.00016   100%  8519  460s\n",
      "    19    22    0.06650    5   14    0.77576    0.00016   100%  7699  469s\n",
      "    23    28    0.00018    5   10    0.77576    0.00016   100%  7340  478s\n",
      "H   29    33                       0.7308171    0.00016   100%  6302  486s\n",
      "    34    39    0.00024    7    8    0.73082    0.00016   100%  5934  495s\n",
      "    40    45    0.00024    8   12    0.73082    0.00016   100%  5750  502s\n",
      "    46    51    0.00024    9   12    0.73082    0.00016   100%  5427  511s\n",
      "    52    58    0.00024    9   15    0.73082    0.00016   100%  5159  517s\n",
      "    59    66    0.00024   10   14    0.73082    0.00016   100%  4848  523s\n",
      "H   63    66                       0.7083452    0.00016   100%  4551  523s\n",
      "    67    77    0.00032   10   18    0.70835    0.00016   100%  4583  531s\n",
      "    78    80    0.00032   11   17    0.70835    0.00016   100%  4158  542s\n",
      "    88    90    0.00032   12   17    0.70835    0.00016   100%  3940  548s\n",
      "   100    98    0.15773   13   22    0.70835    0.00016   100%  3797  562s\n",
      "   111   108    0.00032   14   14    0.70835    0.00016   100%  3608  570s\n",
      "   125   122    0.00032   15   13    0.70835    0.00016   100%  3407  576s\n",
      "   145   146     cutoff   17         0.70835    0.00016   100%  3138  580s\n",
      "H  192   171                       0.7082652    0.00016   100%  2495  584s\n",
      "   201   197    0.56212   28    8    0.70827    0.00016   100%  2408  591s\n",
      "H  241   216                       0.7081852    0.00016   100%  2084  617s\n",
      "   269   229    0.00016    5   10    0.70819    0.00016   100%  2006  639s\n",
      "   306   257    0.00016    6   10    0.70819    0.00016   100%  1819  648s\n",
      "   342   287    0.00016    7   13    0.70819    0.00016   100%  1695  654s\n",
      "   390   287    0.00024    8   12    0.70819    0.00016   100%  1556  669s\n",
      "   406   331    0.00024    9   10    0.70819    0.00016   100%  1563  677s\n",
      "H  433   331                       0.7081052    0.00016   100%  1531  677s\n",
      "   454   346    0.00024   11   10    0.70811    0.00016   100%  1487  696s\n",
      "   480   399    0.00024   12    9    0.70811    0.00016   100%  1416  703s\n",
      "   556   402    0.00024   17   11    0.70811    0.00016   100%  1275  738s\n",
      "   569   419    0.00024   18    7    0.70811    0.00016   100%  1299  767s\n",
      "   601   444    0.00032   20    9    0.70811    0.00016   100%  1350  796s\n",
      "   650   457    0.50915   44    6    0.70811    0.00016   100%  1327  832s\n",
      "   663   460    0.65201   48    7    0.70811    0.00016   100%  1351  869s\n",
      "   686   473    0.00032    5   13    0.70811    0.00016   100%  1402  884s\n",
      "   709   478    0.00031    6   16    0.70811    0.00016   100%  1414  905s\n",
      "   720   495    0.00032    7   12    0.70811    0.00016   100%  1421  916s\n",
      "   737   500    0.00032   11   10    0.70811    0.00016   100%  1451  931s\n",
      "   761   520    0.00032   14   13    0.70811    0.00016   100%  1499  957s\n",
      "   787   542    0.00032   15   14    0.70811    0.00016   100%  1528  979s\n",
      "   819   551    0.00040   19   13    0.70811    0.00016   100%  1544 1000s\n",
      "   828   593    0.00040   22   11    0.70811    0.00016   100%  1601 1018s\n",
      "   893   629    0.00056   38   16    0.70811    0.00016   100%  1546 1068s\n",
      "   957   641    0.00017    7   46    0.70811    0.00016   100%  1570 1109s\n",
      "   981   651    0.00018    8   23    0.70811    0.00016   100%  1613 1127s\n",
      "   991   665    0.00018    9   29    0.70811    0.00016   100%  1653 1168s\n",
      "  1015   673    0.00024   10   14    0.70811    0.00016   100%  1676 1206s\n",
      "  1024   674    0.35972    7    8    0.70811    0.00016   100%  1741 1243s\n",
      "  1026   675    0.11396   26    6    0.70811    0.00016   100%  1737 1256s\n",
      "  1027   676    0.14063   44    7    0.70811    0.00016   100%  1736 1268s\n",
      "  1028   677    0.35979   11    5    0.70811    0.00016   100%  1734 1272s\n",
      "  1030   678    0.50586   23    4    0.70811    0.00016   100%  1730 1276s\n",
      "  1031   679    0.00022   12    4    0.70811    0.00016   100%  1729 1293s\n",
      "  1032   679    0.28138   43    4    0.70811    0.00022   100%  1727 1302s\n",
      "  1033   683    0.00022   12    4    0.70811    0.00022   100%  1747 1337s\n",
      "  1035   686    0.00022   13   40    0.70811    0.00022   100%  1797 1440s\n",
      "  1039   688    0.00022   14   28    0.70811    0.00022   100%  1901 1500s\n",
      "\n",
      "Cutting planes:\n",
      "  Implied bound: 11\n",
      "  MIR: 10\n",
      "  RLT: 1\n",
      "  Relax-and-lift: 8\n",
      "\n",
      "Explored 1042 nodes (2099156 simplex iterations) in 1500.04 seconds (3216.43 work units)\n",
      "Thread count was 8 (of 8 available processors)\n",
      "\n",
      "Solution count 8: 0.708105 0.708185 0.708265 ... 0.798313\n",
      "\n",
      "Time limit reached\n",
      "Best objective 7.081051685393e-01, best bound 2.160781317002e-04, gap 99.9695%\n",
      "WARNING: Loading a SolverResults object with an 'aborted' status, but\n",
      "containing a solution\n",
      "Solver running time: 1501.3946781158447\n",
      "Solver termination condition: maxTimeLimit\n",
      "Valid Tree : Yes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<lsopt.tree.BinNodePenaltyOptimalTreeClassifier at 0x13057ebd0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selection_model.fit(select_X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix :\n",
      "[[707  11]\n",
      " [ 52  37]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.98      0.96       718\n",
      "           1       0.77      0.42      0.54        89\n",
      "\n",
      "    accuracy                           0.92       807\n",
      "   macro avg       0.85      0.70      0.75       807\n",
      "weighted avg       0.91      0.92      0.91       807\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'optimal_tree_fraud.png'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make prediction\n",
    "# selection = SelectFromModel(model, threshold=true_thresh, prefit=True)\n",
    "# select_X_test = selection.transform(X_test)\n",
    "y_pred = selection_model.predict(X=select_X_train)\n",
    "y_pred_prob = selection_model.predict_proba(X=select_X_train)\n",
    "\n",
    "# Check confusion matrix\n",
    "print(\"Confusion Matrix :\")\n",
    "print(confusion_matrix(y_true=y_train,\n",
    "                       y_pred=y_pred))\n",
    "\n",
    "print(classification_report(y_true=y_train,\n",
    "                            y_pred=y_pred))\n",
    "\n",
    "# Plot Optimal Tree\n",
    "feature_names = names\n",
    "class_names = ['Not-Fraud', 'Fraud']\n",
    "\n",
    "dot_data = tree.export_graphviz(selection_model,\n",
    "                                out_file=None,\n",
    "                                feature_names=feature_names,\n",
    "                                class_names=class_names,\n",
    "                                label='all',\n",
    "                                impurity=True,\n",
    "                                node_ids=True,\n",
    "                                filled=True,\n",
    "                                rounded=True,\n",
    "                                leaves_parallel=True,\n",
    "                                special_characters=False)\n",
    "\n",
    "graph = graphviz.Source(dot_data)\n",
    "graph.format = 'png'\n",
    "graph.render(filename='optimal_tree_fraud', directory='', view=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for element in y_pred:\n",
    "    if element == 1:\n",
    "        count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<lsopt.tree.BinNodePenaltyOptimalTreeClassifier object at 0x12ead3e90>\n"
     ]
    }
   ],
   "source": [
    "print(selection_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image('optimal_tree_fraud.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can't pickle local object 'solve_oct_MILP_BIN.<locals>.obj_rule'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\user\\Desktop\\sagata\\Assurance\\optimaltree-master\\boost.ipynb Cell 17\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/sagata/Assurance/optimaltree-master/boost.ipynb#W0sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpickle\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/sagata/Assurance/optimaltree-master/boost.ipynb#W0sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mmodel_pkl\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m file:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/sagata/Assurance/optimaltree-master/boost.ipynb#W0sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     pickle\u001b[39m.\u001b[39;49mdump(selection_model, file)\n",
      "\u001b[1;31mAttributeError\u001b[0m: Can't pickle local object 'solve_oct_MILP_BIN.<locals>.obj_rule'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('model_pkl', 'wb') as file:\n",
    "    pickle.dump(selection_model, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
