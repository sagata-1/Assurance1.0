# Today's report
## Items 1-4 and 6- Getting the GA to work parallely and correctly on the iris dataset- Code in objective.py
1-4. Items 1-4 were basically worked on together. I spent about an hour understanding the VBA code, and realized I needed to refactor an re-order quite a bit. The grad student's code was quite messy, in the sense that there were artifacts left over in her code that were "dummies" that didn't run and were just there in case she needed to user it to check a tree without GA against her benchmark example, while it does run in python. So, what I did was:

### Step 1: Go back to basics
Here, I removed the GA part of the problem, and unpararallezied the solution because I just wanted to see if the current algorithm I had translated matched up with her benchmark example. Turns out, it didn't. After a bit of hunting through the code I realized it was because of two main problems. The first was that I wasn't reading in the feature_data correctly. I'd been reading in the ID column as the first feature of the dataset, which was causing lots of problems with the output (it was misreading error value as accuracy, so the 1.0 and high accuracy values actually meant that the code was predicting everything perfectly wrong). So, I adjusted that. The next problem was just that I hadn't adjusted a bit of the functions like tree-predict to completely 0-index everything, which was required for python, once that was done, then the example matched up with hers.

### Step 2: Adding back GA
So, I added back the serial version of GA, which had a couple of 0-indexing errors that I fixed, and also the a4 function was meant to be returning an int, not a float, so I fixed that problem as well. (The problem was that in her code it was meant to call the class_tree_to_engineering function, but since I couldn't figure what that did at the time, I rewrote the algorithm myself, which made it return a float, so I just adjusted the class_tree_to_engineering function for python and then called it in the a4_function). Once that was fixed, the serial GA matched hers.

### Step 3: Adding parallelism back (and accelerating it)
The biggest challenge with this was that in order to match her GA, the population size and generation size needed to be at least 50. With the initial pararallized algorithm that I had, it would take an absurd amount of time to run that many generations. After poking through the multiprocessing documentation for a bit, and looking over at stack-overflow to get a better sense of how the library was used in practise, I realized that the main problem was that I initialzed the pooling function in estimate_prediction_error, which was called for every generation of the population. That basically introduced a lot of overhead, and the computer just takes too long to re-initialize a pool everytime. So, I adjusted the parallelized algorithm so that the mp.Pool() function was initialized at the beginning of the GA and only closed at the end of the run (there was a momentary bug where I realized that I'd forgotten to remove my pool.close() from the estimate_prediction_error() function, but that was easily fixed). So now, if you run everything, with parallelism 

## Item 5 (and 2)- Node penalty introduction
The main point behind this is to add some kind of penalty so that the GA won't explore certain unreasonable nodes, or overfit on the data with too much reliance on one particular node. With this, there were two options. Either we provide a "tree" penatly, where we penalize trees that are too large (which was what I was doing earlier), or we add a "node" penalty, where random unreasonable values for a particular variable (while correct, would overfit the data) would be penalized. I realized that my old method wouldn't work too well since we aren't actually varying tree depth (it's fixed, just we pick its value.). So, instead, I went with the second route, where basically I noticed at the second depth level, sometimes the algorithm picks tree with a really high variable value, even though that's not needed (eg: sepal widht at 4.8 or something, and recognizes that as virginica anyway at the leaves after that split), so I decided to penalize that node with an alpha of 0.01.

## Item 7- Working with large dataset (over 5000 points, i.e. this was the full fraud dataset)- Code in large.py
Ok, so this was the big one. After a bit of fidgeting and adjusting the data (mainly setting fraud and not fraud to 1 and 2, and then adjusting to count over all features), managed to get a few runs. I started at the standard 150 datapoints just to make sure everything was working, and got an accuracy of 89% (in a 30 second run). Bumped it upto 3000 datapoints, and got 90% accuracy (with a 2 minute and 45 second run). Finally, did 2 runs with 5000 datapoints and got 92% and 90% (with each run taking about 4 minutes). Good news so far! These runs were all without the node penalty, just so that I could get a sense of the first few runs. Then, I added back the node penalty, with an alpha of 0.01, and got an accuracy of 90%.

## Items 8-9- Just rearranging control charts location on notebooks and adding in sources used- Code in google collab notebooks
This was very quick, just a re-arrangement of the Jupyter notebooks and adding in the source links.